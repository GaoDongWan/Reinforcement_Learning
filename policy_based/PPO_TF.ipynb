{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PPO_TF.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"hB-PL2kOuMhx","colab_type":"text"},"source":["### **装载云盘**"]},{"cell_type":"code","metadata":{"id":"W3kk56zyuKF8","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ipnfS2Csyu33","colab_type":"text"},"source":["### **安装tensorflow 2.0**"]},{"cell_type":"code","metadata":{"id":"49C0mzzby3s_","colab_type":"code","colab":{}},"source":["!pip install tensorflow-gpu==2.0.0-beta1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SOpdHh4dAK0w","colab_type":"code","colab":{}},"source":["# tensorlayer 兼容问题\n","!pip install imgaug==0.2.6"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nDeaUCyG_kbP","colab_type":"code","colab":{}},"source":["!pip install tensorlayer"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wgNs2Gd10f73","colab_type":"text"},"source":["### **cd命令**"]},{"cell_type":"code","metadata":{"id":"BBQU9lh_yls5","colab_type":"code","colab":{}},"source":["import os\n","os.chdir(\"drive/My Drive/RL_EA/Policy_Gradient\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3TpgOs8f0lUO","colab_type":"text"},"source":["### **查看当前路径**"]},{"cell_type":"code","metadata":{"id":"c21teJ570bmB","colab_type":"code","outputId":"047115d0-c70b-49af-a0e9-a6e9466528fb","executionInfo":{"status":"ok","timestamp":1568100141374,"user_tz":-480,"elapsed":10168,"user":{"displayName":"Steven Iris","photoUrl":"","userId":"04172472830736813201"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["!pwd"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/RL_EA/Policy_Gradient\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZJvrmYXhvBQ3","colab_type":"text"},"source":["# Proximal Policy Optimization\n","\n","\n","$$\n","on-Policy\\\\\n","discrete\\ \\&\\ continous\n","$$ \n","\n","</br>\n","\n","**PPO对TRPO进行了一定的简化，而且效果和训练速度都比TRPO好。TRPO用到了二阶近似，PPO只用了一阶。**\n","</br>\n","* **TRPO:**\n","\n","$$\n"," \\max_{\\theta}\\mathbb{E}_{s\\sim \\rho_{\\theta_{old}}\\ ,\\ a\\sim \\pi_{\\theta_{old}}}\\left[ \\frac{\\pi_{\\theta}(a \\mid s)}{\\pi_{\\theta_{old}}\\ (a\\mid s)}A_{\\theta_{old}}\\ (s,a)\\right]\\\n","$$\n","</br>\n","$$\n","subject \\ to \\quad \\mathbb{E}_{s\\sim \\rho_{\\theta_{old}}}\\left[ D_{KL}(\\pi_{\\theta_{old}}\\ (\\cdot \\mid s)\\parallel \\pi_{\\theta}(\\cdot \\mid s))\\right] \\le \\delta \n","$$\n","</br>\n","\n","* **PPO:**\n","\n","1. **PPO1（baseline基本都用的PPO2）：**\n","\n","> 关注点在KL惩罚项上，也可以理解成自动调整KL项的系数：\n","\n","$$\n"," \\max_{\\theta}\\mathbb{E}_{s\\sim \\rho_{\\theta_{old}}\\ ,\\ a\\sim \\pi_{\\theta_{old}}}\\left[ \\frac{\\pi_{\\theta}(a \\mid s)}{\\pi_{\\theta_{old}}\\ (a\\mid s)}A_{\\theta_{old}}\\ (s,a)-\\beta D_{KL}(\\pi_{\\theta_{old}}\\ (\\cdot \\mid s)\\parallel \\pi_{\\theta}(\\cdot \\mid s)) \\right]\\\n","$$\n","\n","$$\n","\\text{设} d=\\mathbb{E}_{t}\\big[D_{KL}(\\pi_{\\theta_{old}}\\ (\\cdot \\mid s)\\parallel \\pi_{\\theta}(\\cdot \\mid s)) \\big]\n","$$\n","\n","$$\n","\\text{If}\\ \\ d < d_{targ}\\ ,\\ \\beta \\leftarrow\\ \\beta/2\\ \\ \\\n","$$\n","$$\n","\\text{If}\\ \\ d > d_{targ}\\ ,\\ \\beta \\leftarrow\\ \\beta\\times2\n","$$\n","</br>\n","2. **PPO2：**\n","\n","$$\n","ratio = \\frac{\\pi_{\\theta}(a \\mid s)}{\\pi_{\\theta_{old}}\\ (a\\mid s)}\n","$$\n","\n","$$\n"," \\max_{\\theta}\\mathbb{E}_{s\\sim \\rho_{\\theta_{old}}\\ ,\\ a\\sim \\pi_{\\theta_{old}}} \\big[\\min \\big(ratioA_{\\theta_{old}}\\ (s,a)\\ ,clip(ratio, 1-\\epsilon, 1+\\epsilon)A_{\\theta_{old}}\\ (s,a)\\big)\\big]\\\n","$$\n","</br>\n","value（critic）的更新方式同一般的MSE。"]},{"cell_type":"code","metadata":{"id":"zBschy8n8Ilx","colab_type":"code","colab":{}},"source":["import os\n","import time\n","\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","\n","import gym\n","import tensorflow as tf\n","import tensorflow_probability as tfp\n","import tensorlayer as tl"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E2yb3b3UBUbE","colab_type":"code","colab":{}},"source":["##### hyper parameters #####\n","ENV_NAME = 'Pendulum-v0' # CartPole-v0 Pendulum-v0\n","RANDOMSEED = 1\n","EP_MAX = 2000\n","EP_LEN = 200\n","GAMMA = 0.9 # reward折扣因子\n","A_LR = 0.0002 # 0.0001\n","C_LR = 0.0003 # 0.0002\n","BATCH = 32\n","A_UPDATE_STEPS = 10 # actor 更新steps\n","C_UPDATE_STEPS = 10 # critic 更新steps\n","S_DIM, A_DIM = 3, 1 # 提前给出env的状态，动作维度\n","EPS = 1e-8 # epsilon\n","\n","METHOD = [\n","  dict(name='kl_pen', kl_target=0.01, lam=0.5), # KL penalty\n","  dict(name='clip', epsilon=0.15), # 裁剪参数，源代码认为0.2挺好\n","][1]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0BhJmhy-0Jhl","colab_type":"code","colab":{}},"source":["# PPO\n","class PPO:\n","\n","  def __init__(self):\n","    # critic\n","    tfs = tl.layers.Input([None, S_DIM], tf.float32, 'state')\n","    l1 = tl.layers.Dense(100, tf.nn.relu)(tfs)\n","    v = tl.layers.Dense(1)(l1)\n","    self.critic = tl.models.Model(tfs, v)\n","    self.critic.train()\n","\n","    # actor\n","    self.actor = self._build_anet('pi1', trainable=True)\n","    self.actor_old = self._build_anet('oldpi1', trainable=False)\n","    self.actor_opt = tf.optimizers.Adam(A_LR)\n","    self.critic_opt = tf.optimizers.Adam(C_LR)\n","    \n","  \n","  def _build_anet(self, name, trainable):\n","    tfs = tl.layers.Input([None, S_DIM], tf.float32, name + '_state')\n","    l1 = tl.layers.Dense(100, tf.nn.relu, name=name + '_l1')(tfs)\n","    a = tl.layers.Dense(A_DIM, tf.nn.tanh, name=name + '_a')(l1)\n","    # 连续空间用μ和σ表示\n","    mu = tl.layers.Lambda(lambda x: x * 2, name=name + '_lambda')(a)\n","    sigma = tl.layers.Dense(A_DIM, tf.nn.softplus, name=name + '_sigma')(l1) # softplus 平滑的relu\n","    model = tl.models.Model(tfs, [mu, sigma], name)\n","\n","    if trainable:\n","      model.train()\n","    else:\n","      model.eval()\n","    return model\n","\n","  def a_train(self, tfs, tfa, tfadv):\n","    '''\n","    状态(观察)，动作，adv(优势)值\n","    '''\n","    tfs = np.array(tfs, np.float32)\n","    tfa = np.array(tfa, np.float32)\n","    tfadv = np.array(tfadv, np.float32)\n","    with tf.GradientTape() as tape:\n","      mu, sigma = self.actor(tfs)\n","      pi = tfp.distributions.Normal(mu, sigma)\n","\n","      mu_old, sigma_old = self.actor_old(tfs)\n","      oldpi = tfp.distributions.Normal(mu_old, sigma_old)\n","\n","      ratio = pi.prob(tfa) / (oldpi.prob(tfa) + EPS)\n","      surr = ratio * tfadv\n","      if METHOD['name'] == 'kl_pen':\n","        tflam = METHOD['lam']\n","        kl = tfp.distributions.kl_divergence(oldpi, pi)\n","        kl_mean = tf.reduce_mean(kl)\n","        aloss = -(tf.reduce_mean(surr - tflam * kl))\n","      else:\n","        aloss = -tf.reduce_mean(\n","          tf.minimum(surr,\n","                tf.clip_by_value(ratio, 1. - METHOD['epsilon'], 1. + METHOD['epsilon']) * tfadv)\n","        )\n","    a_grad = tape.gradient(aloss, self.actor.trainable_weights)\n","\n","    self.actor_opt.apply_gradients(zip(a_grad, self.actor.trainable_weights))\n","\n","    if METHOD['name'] == 'kl_pen':\n","      return kl_mean\n","\n","  def update_old_pi(self):\n","    for p, oldp in zip(self.actor.trainable_weights, self.actor_old.trainable_weights):\n","      oldp.assign(p)\n","\n","  def c_train(self, tfdc_r, s):\n","    '''\n","    tfdc_r: 累计reward\n","    '''\n","    tfdc_r = np.array(tfdc_r, np.float32)\n","    with tf.GradientTape() as tape:\n","      v = self.critic(s)\n","      adv = tfdc_r - v\n","      closs = tf.reduce_mean(tf.square(adv))\n","    c_grad = tape.gradient(closs, self.critic.trainable_weights)\n","    self.critic_opt.apply_gradients(zip(c_grad, self.critic.trainable_weights))\n","\n","  def cal_adv(self, tfs, tfdc_r):\n","    '''\n","    状态， 累计reward -> 优势\n","    '''\n","    tfdc_r = np.array(tfdc_r, np.float32)\n","    adv = tfdc_r - self.critic(tfs)\n","    return adv.numpy()\n","\n","  def update(self, s, a, r):\n","    s, a, r = s.astype(np.float32), a.astype(np.float32), r.astype(np.float32)\n","\n","    self.update_old_pi()\n","    adv = self.cal_adv(s, r)\n","    # adv = (adv-adv.mean())/(adv.std()+1e-6)\n","\n","    # actor\n","    if METHOD['name'] == 'kl_pen':\n","      for _ in range(A_UPDATE_STEPS):\n","        kl = self.a_train(s, a, adv)\n","        if kl > 4 * METHOD['kl_target']: # google论文里的\n","          break\n","      if kl < METHOD['kl_target'] / 1.5: # OpenAI论文里的\n","        METHOD['lam'] /= 2\n","      elif kl > METHOD['kl_target'] * 1.5:\n","        METHOD['lam'] *= 2\n","      METHOD['lam'] = np.clip(METHOD['lam'], 1e-4, 10)\n","    else:\n","      for _ in range(A_UPDATE_STEPS):\n","        self.a_train(s, a, adv)\n","      \n","    # critic\n","    for _ in range(C_UPDATE_STEPS):\n","      self.c_train(r, s)\n","\n","  def choose_action(self, s):\n","    s = s[np.newaxis, :].astype(np.float32)\n","    mu, sigma = self.actor(s)\n","    pi = tfp.distributions.Normal(mu, sigma)\n","    a = tf.squeeze(pi.sample(1), axis=0)[0]\n","    return np.clip(a, -2, 2)\n","\n","  def get_v(self, s):\n","    s = s.astype(np.float32)\n","    if s.ndim < 2:\n","      s = s[np.newaxis, :]\n","    return self.critic(s)[0, 0] # ??\n","\n","  def save_ckpt(self):\n","    if not os.path.exists('model'):\n","      os.makedirs('model')\n","    tl.files.save_weights_to_hdf5('model/ppo_actor.hdf5', self.actor)\n","    tl.files.save_weights_to_hdf5('model/ppo_actor_old.hdf5', self.actor_old)\n","    tl.files.save_weights_to_hdf5('model/ppo_critic.hdf5', self.critic)\n","\n","  def load_ckpt(self):\n","    tl.files.load_hdf5_to_weights_in_order('model/ppo_actor.hdf5', self.actor)\n","    tl.files.load_hdf5_to_weights_in_order('model/ppo_actor_old.hdf5', self.actor_old)\n","    tl.files.load_hdf5_to_weights_in_order('model/ppo_critic.hdf5', self.critic)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iTaFyrYJAf6D","colab_type":"text"},"source":["### **Main （初始化，训练，测试）**"]},{"cell_type":"code","metadata":{"id":"p6-k0QU2zYV6","colab_type":"code","outputId":"c0d52a99-2595-4509-bc92-dba918f242fc","executionInfo":{"status":"ok","timestamp":1568100168171,"user_tz":-480,"elapsed":3760,"user":{"displayName":"Steven Iris","photoUrl":"","userId":"04172472830736813201"}},"colab":{"base_uri":"https://localhost:8080/","height":251}},"source":["env = gym.make(ENV_NAME).unwrapped\n","\n","env.seed(RANDOMSEED)\n","np.random.seed(RANDOMSEED)\n","tf.random.set_seed(RANDOMSEED)\n","\n","ppo = PPO()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[TL] Input  state: [None, 3]\n","[TL] Dense  dense_1: 100 relu\n","[TL] Dense  dense_2: 1 No Activation\n","[TL] Input  pi1_state: [None, 3]\n","[TL] Dense  pi1_l1: 100 relu\n","[TL] Dense  pi1_a: 1 tanh\n","[TL] Lambda  pi1_lambda: func: <function PPO._build_anet.<locals>.<lambda> at 0x7f55cefbc620>, len_weights: 0\n","[TL] Dense  pi1_sigma: 1 softplus\n","[TL] Input  oldpi1_state: [None, 3]\n","[TL] Dense  oldpi1_l1: 100 relu\n","[TL] Dense  oldpi1_a: 1 tanh\n","[TL] Lambda  oldpi1_lambda: func: <function PPO._build_anet.<locals>.<lambda> at 0x7f55cefbc9d8>, len_weights: 0\n","[TL] Dense  oldpi1_sigma: 1 softplus\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oOSaAHZdzrlN","colab_type":"code","colab":{}},"source":["# train\n","all_ep_r = []\n","for ep in range(EP_MAX):\n","  s = env.reset()\n","  buffer_s, buffer_a, buffer_r = [], [], []\n","  ep_r = 0\n","  t0 = time.time()\n","  for t in range(EP_LEN): # 一个episode\n","    a = ppo.choose_action(s)\n","    s_, r, done, _ = env.step(a)\n","    buffer_s.append(s)\n","    buffer_a.append(a)\n","    buffer_r.append((r + 8) / 8) # 源代码认为这么norm能有效\n","    s = s_\n","    ep_r += r\n","\n","    # update\n","    if (t + 1) % BATCH == 0 or t == EP_LEN - 1:\n","      v_s_ = ppo.get_v(s_)\n","      discounted_r = []\n","      for r in buffer_r[::-1]:\n","        v_s_ = r + GAMMA * v_s_\n","        discounted_r.append(v_s_)\n","      discounted_r.reverse()\n","\n","      bs, ba, br = np.vstack(buffer_s), np.vstack(buffer_a), np.array(discounted_r)[:, np.newaxis]\n","      buffer_s, buffer_a, buffer_r = [], [], []\n","      ppo.update(bs, ba, br)\n","  if ep == 0:\n","    all_ep_r.append(ep_r)\n","  else:\n","    all_ep_r.append(all_ep_r[-1] * 0.9 + ep_r * 0.1)\n","  print(\n","    'Episode:{}/{} | Episode Reward:{:.4f} | Running Time:{:.4f}'.format(\n","        ep, EP_MAX, ep_r,\n","        time.time() - t0\n","    )\n","  )\n","\n","  plt.ion()\n","  plt.cla()\n","  plt.title('PPO')\n","  plt.plot(np.arange(len(all_ep_r)), all_ep_r)\n","  plt.ylim(-2000, 0)\n","  plt.xlabel('Episode')\n","  plt.ylabel('Moving averaged episode reward')\n","  plt.show()\n","  plt.pause(0.1)\n","  if ep % 500 == 0:\n","    ppo.save_ckpt()\n","ppo.save_ckpt()\n","plt.ioff()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wQKi1hTo3Cda","colab_type":"code","colab":{}},"source":["# test\n","ppo.load_ckpt()\n","while True:\n","  s = env.reset()\n","  for i in range(EP_LEN):\n","    # env.render()\n","    s, r, done, _ = env.step(ppo.choose_action(s))\n","    if done:\n","      break"],"execution_count":0,"outputs":[]}]}