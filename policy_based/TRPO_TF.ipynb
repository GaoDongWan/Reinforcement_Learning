{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TRPO_TF.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"hB-PL2kOuMhx","colab_type":"text"},"source":["### **装载云盘**"]},{"cell_type":"code","metadata":{"id":"W3kk56zyuKF8","colab_type":"code","outputId":"8bf6846a-be46-46a9-b33e-f0d6a8a48863","executionInfo":{"status":"ok","timestamp":1567734693872,"user_tz":-480,"elapsed":48911,"user":{"displayName":"Steven Iris","photoUrl":"","userId":"04172472830736813201"}},"colab":{"base_uri":"https://localhost:8080/","height":127}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ipnfS2Csyu33","colab_type":"text"},"source":["### **安装tensorflow 2.0**"]},{"cell_type":"code","metadata":{"id":"49C0mzzby3s_","colab_type":"code","colab":{}},"source":["!pip install tensorflow-gpu==2.0.0-beta1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SOpdHh4dAK0w","colab_type":"code","colab":{}},"source":["# tensorlayer 兼容问题\n","!pip install imgaug==0.2.6"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nDeaUCyG_kbP","colab_type":"code","colab":{}},"source":["!pip install tensorlayer"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wgNs2Gd10f73","colab_type":"text"},"source":["### **cd命令**"]},{"cell_type":"code","metadata":{"id":"BBQU9lh_yls5","colab_type":"code","colab":{}},"source":["import os\n","os.chdir(\"drive/My Drive/RL_EA/Actor_Critic\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3TpgOs8f0lUO","colab_type":"text"},"source":["### **查看当前路径**"]},{"cell_type":"code","metadata":{"id":"c21teJ570bmB","colab_type":"code","outputId":"ca73c2de-228e-4625-e997-00d798710a3c","executionInfo":{"status":"ok","timestamp":1567734895562,"user_tz":-480,"elapsed":4040,"user":{"displayName":"Steven Iris","photoUrl":"","userId":"04172472830736813201"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["!pwd"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/RL_EA/Actor_Critic\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZJvrmYXhvBQ3","colab_type":"text"},"source":["# Trust Region Policy Optimization\n","\n","\n","$$\n","on-Policy\\\\\n","discrete\\ \\&\\ continous\n","$$ \n","\n","\n","常规PG的新旧策略在参数空间上接近，但可能会导致性能相差甚远。TRPO主要思想就是通过**KL散度的限制**来让更新操作保持**回报函数单调不减**。\n","\n","* **原文中定义的回报函数，Q，V，A：**\n","\n","\n","$$\n","\\eta(\\pi)=\\mathbb{E}{s_0,a_0,\\ldots}\\left[ \\sum_{t=0}^{\\infty}\\gamma^tr(s_t) \\right] \n","$$\n","\n","$$\n","Q_\\pi (s_t,a_t)=\\mathbb{E}{s_{t+1},a_{t+1},\\ldots}\\left[ \\sum_{l=0}^{\\infty}\\gamma^l r(s_{t+l}) \\right]\n","$$\n","\n","$$\n","V_{\\pi}(s_t)=\\mathbb{E}{a_t,s_{t+1},\\ldots} \\left[\\sum_{l=0}^{\\infty}\\gamma^l r(s_{t+l}) \\right] \n","$$\n","\n","$$\n","\\begin{equation} \\begin{split} A_\\pi (s,a)&=Q_\\pi(s,a)-V_\\pi(s) \\\\&= r(s)+\\gamma V_\\pi(s')-V_\\pi(s) \\end{split} \\end{equation}\n","$$\n","</br>\n","* 写成增量形式，要保持策略效果单调不减只要**保证后面的哪一项非负即可**(有证明)。\n","\n","$$\n","\\eta(\\widetilde{\\pi})=\\eta(\\pi)+\\mathbb{E}{s_0,a_0,\\ldots \\sim \\widetilde{\\pi}}\\left[ \\sum_{t=0}^{\\infty}\\gamma^t A_\\pi(s_t,a_t) \\right] \n","$$\n","</br>\n","* 各种近似操作：\n","\n","$$\n","\\mbox{设}\\  \\rho_\\pi (s)=P(s_0=s \\mid \\pi)+\\gamma P(s_1=s \\mid \\pi)+\\gamma^2 P(s_2=s \\mid \\pi)+\\dots=\\sum_{t=0}^\\infty \\gamma^t  P(s_t=s \\mid \\pi) \n","$$\n","\n","$$\n","\\eta(\\widetilde{\\pi}) = \\eta(\\pi)+ \\sum_s \\sum_{t=0}^\\infty \\gamma^t  P(s_t=s \\mid \\widetilde{\\pi})  \\sum_a \\widetilde{\\pi}(a_t=a \\mid s_t) A_\\pi(s_t,a_t) \n","$$\n","\n","$$\n","\\eta(\\widetilde{\\pi}) = \\eta(\\pi)+  \\sum_s  \\rho_\\widetilde{\\pi}(s) \\sum_a \\widetilde{\\pi}(a \\mid s) A_\\pi(s,a)\n","$$\n","\n","* 由于$\\widetilde{\\pi}$是要学的新策略，比较难计算，因此用旧策略$\\pi$来近似，得到：\n","\n","$$\n","L_\\pi(\\widetilde{\\pi})=\\eta(\\pi)+  \\sum_s  \\rho_{\\pi}(s) \\sum_a \\widetilde{\\pi}(a \\mid s) A_\\pi(s,a)\n","$$\n","\n","* 由于$L_{\\pi}()$和$\\eta()$在$\\pi$处一阶导相同，因此在新旧策略相近的情况下其效果相同。经过作者的证明得到关键的不等式：\n","\n","$$\n","\\eta(\\pi_{new})\\ge L_{\\pi_{old}}\\ (\\pi_{new})-C\\cdot D_{KL}^{max}(\\pi_{old},\\pi_{new}) \n","$$\n","\n","$$\n","C=\\frac{4\\epsilon \\gamma}{(1-\\gamma)^2} \n","$$\n","\n","$$\n","\\epsilon=\\max_{s,a}\\left | A_\\pi(s,a) \\right|\n","$$\n","\n","* 换下角标：\n","\n","$$\n","\\eta(\\pi)\\ge L_{\\pi_{i}}(\\pi)-C\\cdot D_{KL}^{max}(\\pi_{i},\\pi) \n","$$\n","\n","* 并用M来表示下界：\n","\n","$$\n","M_i(\\pi)=L_{\\pi_i}(\\pi)-C\\cdot D_{KL}^{max}(\\pi_{i},\\pi)\n","$$\n","\n","$$\n","\\eta(\\pi_{i+1})\\ge M_i(\\pi_{i+1})\n","$$\n","\n","$$\n","\\eta(\\pi_i)=M_i(\\pi_i)=L_{\\pi_i}(\\pi_i) \n","$$\n","\n","* 得到：\n","\n","$$\n","\\eta(\\pi_{i+1})-\\eta(\\pi_i)\\ge M_i(\\pi_{i+1})-M_i(\\pi_i) \n","$$\n","\n","* **那么$\\pi_{i+1}$只要满足下面即可保证单调性：**\n","\n","$$\n","\\pi_{i+1}=arg\\max_\\pi \\left[  L_{\\pi_{i}}(\\pi)-C\\cdot D_{KL}^{max}(\\pi_{i}\\parallel \\pi)\\right] \n","$$\n","\n","* 由于$KL$的max计算复杂，用mean来近似；$C$会限制更新步幅，将其改成有约束的优化；同时用重要性采样来消除对动作概率的求和，**最终将问题改为：**\n","\n","$$\n"," \\max_{\\theta}\\mathbb{E}_{s\\sim \\rho_{\\theta_{old}}\\ ,\\ a\\sim \\pi_{\\theta_{old}}}\\left[ \\frac{\\pi_{\\theta}(a \\mid s)}{\\pi_{\\theta_{old}}\\ (a\\mid s)}A_{\\theta_{old}}\\ (s,a)\\right]\\\n","$$\n","</br>\n","$$\n","subject \\ to \\quad \\mathbb{E}_{s\\sim \\rho_{\\theta_{old}}}\\left[ D_{KL}(\\pi_{\\theta_{old}}\\ (\\cdot \\mid s)\\parallel \\pi_{\\theta}(\\cdot \\mid s))\\right] \\le \\delta \n","$$\n","</br></br>\n","在实际优化中用的是**共轭梯度**，而不是SGD（这里用到了**拉格朗是对偶**的方法，因为原问题是个有约束的优化问题，从而想到拉格朗日优化方法）。\n","</br>\n","**KL散度可用Fisher矩阵来近似**（极值处二阶导相等），用到**Hession Free**（H阵总是以**Hv**的形式出现，通过两次反向传播来计算）方式减小计算量，得到最终的policy更新公式，$\\alpha_{i}$是**linear search**的系数（不断地/2，直到满足约束，或者放弃本次更新）,**得到policy参数的更新方式：**\n","\n","$$\n","\\theta_{k+1}=\\theta_{k}+\\alpha^{j}\\sqrt{\\frac{2\\delta}{g^{T}H^{-1}g}}H^{-1}g\n","$$\n","\n","对critic网络参数进行更新的方式就是用**MSE（targ-eval），这里的targ用到了GAE（权衡reward考虑多少未来步长）来计算**。"]},{"cell_type":"code","metadata":{"id":"zBschy8n8Ilx","colab_type":"code","colab":{}},"source":["import os\n","import time\n","import copy\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import scipy.signal\n","\n","import gym\n","import tensorflow as tf\n","import tensorflow_probability as tfp\n","import tensorlayer as tl\n","from gym.spaces import Box, Discrete"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E2yb3b3UBUbE","colab_type":"code","colab":{}},"source":["##### hyper parameters #####\n","ENV_NAME = 'Pendulum-v0' # CartPole-v0 Pendulum-v0\n","SEED = 1\n","GAMMA = 0.99 # reward折扣因子\n","EPOCHS = 500 # 500\n","STEPS_PER_EPOCH = 4000 # 4000\n","HIDDEN_SIZES = [64] * 2\n","\n","DELTA = 0.01 # 更新TRPO的KL散度的限制\n","VF_LR = 1e-3 # value function 学习率\n","TRAIN_V_ITERS = 80 # 每个epoch中，走这些步就进行梯度下降\n","DAMPING_COEFF = 0.1 # 数值稳定性相关\n","CG_ITERS = 10\n","BACKTRACK_ITERS = 10\n","BACKTRACK_COEFF = 0.8\n","LAM = 0.97 # GAE-lambda\n","MAX_EP_LEN = 1000 # 1000\n","SAVE_FREQ = 10\n","EPS = 1e-8 # epsilon"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sDyLMzsH7ml5","colab_type":"code","colab":{}},"source":["# Utils\n","\n","def combined_shape(length, shape=None):\n","  \"\"\"\n","  合并length和shape\n","  \"\"\"\n","  if shape is None:\n","    return length,\n","  return (length, shape) if np.isscalar(shape) else (length, *shape)\n","\n","def keys_as_sorted_list(dict):\n","  return sorted(list(dict.keys()))\n","\n","def values_as_sorted_list(dict):\n","  return [dict[k] for k in keys_as_sorted_list(dict)]\n","\n","def input_layer(dim=None):\n","  return tl.layers.Input(dtype=tf.float32, shape=combined_shape(None, dim))\n","\n","def input_layers(*args):\n","  return [input_layer(dim) for dim in args]\n","\n","def input_layer_from_space(space):\n","  '''\n","  判断输入空间(状态)是连续还是离散\n","  '''\n","  if isinstance(space, Box):\n","    return input_layer(space.shape)\n","  elif isinstance(space, Discrete):\n","    return tl.layers.Input(dtype=tf.int32, shape=(None, ))\n","  raise NotImplementedError\n","\n","def input_layers_from_spaces(*args):\n","  return [input_layer_from_space(space) for space in args]\n","\n","def mlp(x, hidden_sizes=(32, ), activation=tf.tanh, output_activation=None):\n","  '''\n","  根据hidden_size:[32,32,4]建网络，for n-1层，return最后一层\n","  '''\n","  for h in hidden_sizes[:-1]:\n","    x = tl.layers.Dense(n_units=h, act=activation)(x)\n","  return tl.layers.Dense(n_units=hidden_sizes[-1], act=output_activation)(x)\n","\n","def get_vars(model: tl.models.Model):\n","  return model.trainable_weights\n","\n","def count_vars(model: tl.models.Model):\n","  v = get_vars(model)\n","  return sum([np.prod(var.shape.as_list()) for var in v])\n","\n","def gaussian_likelihood(x, mu, log_std):\n","  '''\n","  计算正态分布 对数似然函数\n","  '''\n","  pre_sum = -0.5 * (((x - mu) / (tf.exp(log_std) + EPS))**2 + 2 * log_std + np.log(2 * np.pi))\n","  return tf.reduce_sum(pre_sum, axis=1)\n","\n","def diagonal_gaussian_kl(mu0, log_std0, mu1, log_std1):\n","  '''\n","  计算高斯(正态)分布之间的KL散度\n","  '''\n","  var0, var1 = tf.exp(2 * log_std0), tf.exp(2 * log_std1)\n","  pre_sum = 0.5 * (((mu1 - mu0)**2 + var0) / (var1 + EPS) - 1) +log_std1 -log_std0\n","  all_kls = tf.reduce_sum(pre_sum, axis=1)\n","  return tf.reduce_mean(all_kls)\n","\n","def categorical_kl(logp0, logp1):\n","  '''\n","  all_kls：KL散度，mean返回论文中用的平均KL散度\n","  '''\n","  all_kls = tf.reduce_sum(tf.exp(logp1) * (logp1 - logp0), axis=1)\n","  return tf.reduce_mean(all_kls)\n","\n","def flat_concat(xs):\n","  return tf.concat([tf.reshape(x, (-1, )) for x in xs], axis=0)\n","\n","def assign_params_from_flat(x, params):\n","  flat_size = lambda p: int(np.prod(p.shape.as_list()))\n","  splits = tf.split(x, [flat_size(p) for p in params])\n","  new_params = [tf.reshape(p_new, p.shape) for p, p_new in zip(params, splits)]\n","  return tf.group([p.assign(p_new) for p, p_new in zip(params, new_params)])\n","\n","def discount_cumsum(x, discount):\n","  return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yIiUGO1QCie8","colab_type":"code","colab":{}},"source":["# Policies\n","class MlpCategoricalPolicy:\n","  \"\"\"\n","  离散动作\n","  \"\"\"\n","  def __init__(self, x, a, hidden_size, activation, output_activation):\n","    self.act_dim = a.n\n","    x = input_layer_from_space(x)\n","    logits = mlp(x, list(hidden_size) + [self.act_dim], activation, None)\n","    self.model = tl.models.Model(x, logits)\n","    self.model.train()\n","\n","  def cal_outputs_0(self, states):\n","    states = states.astype(np.float32)\n","    logits = self.model(states)\n","    logp_all = tf.nn.log_softmax(logits)\n","    # pi = tf.squeeze(tfp.distributions.Multinomial(1, logits), axis=1) \n","    # 加了._sample_n(1)\n","    # pi = tf.squeeze(tfp.distributions.Multinomial(1, logits)._sample_n(1), axis=1)\n","    pi = tf.squeeze(tfp.distributions.Multinomial(1, logits)._sample_n(1))\n","    if pi.ndim == 1:\n","      pi = tf.expand_dims(pi, axis=0)    \n","    pi = tf.argmax(pi, axis=1)\n","    logp_pi = tf.reduce_sum(tf.one_hot(pi, depth=self.act_dim) * logp_all, axis=1)\n","    info = {'logp_all': logp_all}\n","    return pi, logp_pi, info, logp_all\n","\n","  def cal_outputs_1(self, states, actions, old_logp_all):\n","    pi, logp_pi, info, logp_all = self.cal_outputs_0(states)\n","    logp = tf.reduce_sum(tf.one_hot(actions, depth=self.act_dim) * logp_all, axis=1)\n","    d_kl = categorical_kl(logp_all, old_logp_all)\n","\n","    info_phs = {'logp_all': logp_all}\n","    return pi, logp, logp_pi, info, info_phs, d_kl\n","\n","class MlpGaussianPolicy:\n","  '''\n","  连续动作\n","  '''\n","  def __init__(self, x, a, hidden_size, activation, output_activation):\n","    act_dim = a.shape[0]\n","\n","    x = input_layer_from_space(x)\n","    mu = mlp(x, list(hidden_size) + [act_dim], activation, output_activation)\n","    self.model = tl.models.Model(x, mu)\n","    self.model.train()\n","\n","    self._log_std = tf.Variable(-0.5 * np.ones(act_dim, dtype=np.float32))\n","    self.model.trainable_weights.append(self._log_std)\n","\n","  def cal_outputs_0(self, states):\n","    states = states.astype(np.float32)\n","    mu = self.model(states)\n","    std = tf.exp(self._log_std)\n","    pi = mu + tf.random.normal(tf.shape(mu)) * std\n","    logp_pi = gaussian_likelihood(pi, mu, self._log_std)\n","\n","    info = {'mu': mu, 'log_std': self._log_std}\n","\n","    return pi, logp_pi, info, mu, self._log_std\n","\n","  def cal_outputs_1(self, states, actions, old_log_std_ph, old_mu_ph):\n","    pi, logp_pi, info, mu, log_std = self.cal_outputs_0(states)\n","    logp = gaussian_likelihood(actions, mu, log_std)\n","    d_kl = diagonal_gaussian_kl(mu, log_std, old_mu_ph, old_log_std_ph)\n","\n","    info_phs = {'mu': old_mu_ph, 'log_std': old_log_std_ph}\n","\n","    return pi, logp, logp_pi, info, info_phs, d_kl"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QzEmpxB4DYvw","colab_type":"code","colab":{}},"source":["# Actor-Critics\n","def mlp_actor_critic(\n","    x: 'env.observation_space', a: 'env.action_space', hidden_sizes=(64, 64), activation=tf.tanh,\n","    output_activation=None\n","):\n","  '''\n","  创建actor和critic\n","  '''\n","  if isinstance(a, Box):\n","    actor = MlpGaussianPolicy(x, a, hidden_sizes, activation, output_activation)\n","  elif isinstance(a, Discrete):\n","    actor = MlpCategoricalPolicy(x, a, hidden_sizes, activation, output_activation)\n","  else:\n","    raise ValueError('action space type error')\n","\n","  class Critic:\n","    \n","    def __init__(self, obs_space, hidden_layer_sizes, activation_funcs):\n","      inputs = input_layer_from_space(obs_space)\n","      self.model = tl.models.Model(inputs, mlp(inputs, list(hidden_layer_sizes) + [1], activation_funcs, None))\n","      self.model.train()\n","\n","    def critic_cal_func(self, states):\n","      states = states.astype(np.float32)\n","      return tf.squeeze(self.model(states), axis=1)\n","\n","  critic = Critic(x, hidden_sizes, activation)\n","\n","  return actor, critic"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rR8d1MM1XJf0","colab_type":"code","colab":{}},"source":["# GAEBuffer\n","class GAEBuffer:\n","  '''\n","  经验池，用Generalized Adavantage Esitmation计算优势值\n","  '''\n","\n","  def __init__(self, obs_dim, act_dim, size, info_shapes, gamma=0.99, lam=0.95):\n","    self.obs_buf = np.zeros(combined_shape(size, obs_dim), dtype=np.float32)\n","    self.act_buf = np.zeros(combined_shape(size, act_dim), dtype=np.float32)\n","    self.adv_buf = np.zeros(size, dtype=np.float32)\n","    self.rew_buf = np.zeros(size, dtype=np.float32)\n","    self.ret_buf = np.zeros(size, dtype=np.float32)\n","    self.val_buf = np.zeros(size, dtype=np.float32)\n","    self.logp_buf = np.zeros(size, dtype=np.float32)\n","    self.info_bufs = {k: np.zeros([size] + list(v), dtype=np.float32) for k, v in info_shapes.items()}\n","    self.sorted_info_keys = keys_as_sorted_list(self.info_bufs)\n","    self.gamma, self.lam = gamma, lam\n","    self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n","\n","  def store(self, obs, act, rew, val, logp, info):\n","    assert self.ptr < self.max_size\n","    self.obs_buf[self.ptr] = obs\n","    self.act_buf[self.ptr] = act\n","    self.rew_buf[self.ptr] = rew\n","    self.val_buf[self.ptr] = val\n","    self.logp_buf[self.ptr] = logp\n","    for i, k in enumerate(self.sorted_info_keys):\n","      self.info_bufs[k][self.ptr] = info[i]\n","    self.ptr += 1\n","\n","  def finish_path(self, last_val=0):\n","    '''\n","    在轨迹结束时用到，计算整个轨迹中的GAE，折扣reward等\n","\n","    如果轨迹结束了，那么last_val=0\n","    '''\n","    path_slice = slice(self.path_start_idx, self.ptr)\n","    rews = np.append(self.rew_buf[path_slice], last_val)\n","    vals = np.append(self.val_buf[path_slice], last_val)\n","\n","    # GAE 计算\n","    deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n","    self.adv_buf[path_slice] = discount_cumsum(deltas, self.gamma * self.lam)\n","\n","    # rewards-to-go targets的value func\n","    self.ret_buf[path_slice] = discount_cumsum(rews, self.gamma)[:-1]\n","\n","    self.path_start_idx = self.ptr\n","\n","  def get(self):\n","    '''\n","    在epoch结束时使用，得到所有buffer里的数据\n","    '''\n","    assert self.ptr == self.max_size\n","    self.ptr, self.path_start_idx = 0, 0\n","\n","    # 正规化advantage\n","    adv_mean, adv_std = np.mean(self.adv_buf), np.std(self.adv_buf)\n","    self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n","    return [self.obs_buf, self.act_buf, self.adv_buf, self.ret_buf, self.logp_buf]\\\n","     + values_as_sorted_list(self.info_bufs)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KNTlhKs9EGnJ","colab_type":"code","colab":{}},"source":["## TRPO\n","class TRPO:\n","\n","  def __init__(self, obs_space, act_space):\n","\n","    obs_dim = obs_space.shape\n","    act_dim = obs_space.shape\n","\n","    self.actor, self.critic = mlp_actor_critic(obs_space, act_space, HIDDEN_SIZES)\n","\n","    if isinstance(act_space, Box):\n","      act_dim = env.action_space.shape[0]\n","      info_shapes = {'mu': [act_dim], 'log_std': [act_dim]}\n","    elif isinstance(env.action_space, Discrete):\n","      act_dim = env.action_space.n\n","      info_shapes = {'logp_all': [act_dim]}\n","    else:\n","      raise Exception('info_shape error')\n","\n","    self.buf = GAEBuffer(obs_dim, act_dim, STEPS_PER_EPOCH, info_shapes, GAMMA, LAM)\n","\n","    self.critic_optimizer = tf.optimizers.Adam(learning_rate=VF_LR)\n","\n","  def get_action_ops(self, states):\n","    pi, logp_pi, info, *_ = self.actor.cal_outputs_0(states)\n","    v = self.critic.critic_cal_func(states)\n","    res0 = [pi, v, logp_pi] + values_as_sorted_list(info)\n","    res = []\n","    for i in res0:\n","      res.append(i + 0) # transfer to tensor\n","    return res\n","\n","  def pi_loss(self, inputs):\n","    # pi_loss\n","    x_ph, a_ph, adv_ph, ret_ph, logp_old_ph, *info_values = inputs\n","\n","    pi, logp, logp_pi, info, info_phs, d_kl = self.actor.cal_outputs_1(x_ph, a_ph, *info_values)\n","    ratio = tf.exp(logp - logp_old_ph) # pi(a|s) / pi_old(a|s)\n","    pi_loss = -tf.reduce_mean(ratio * adv_ph)\n","    return pi_loss\n","\n","  def v_loss(self, inputs):\n","    # v_loss\n","    x_ph, a_ph, adv_ph, ret_ph, logp_old_ph, *info_values = inputs\n","    v = self.critic.critic_cal_func(x_ph)\n","    v_loss = tf.reduce_mean((ret_ph - v)**2)\n","    return v_loss\n","\n","  def train_vf(self, inputs):\n","    # 训练 v (critic)\n","    with tf.GradientTape() as tape:\n","      loss = self.v_loss(inputs)\n","    grad = tape.gradient(loss, self.critic.model.trainable_weights)\n","    self.critic_optimizer.apply_gradients(zip(grad, self.critic.model.trainable_weights))\n","\n","  # 用来做共轭梯度\n","  def gradient(self, inputs):\n","    pi_params = self.actor.model.trainable_weights\n","    with tf.GradientTape() as tape:\n","      loss = self.pi_loss(inputs)\n","    grad = tape.gradient(loss, pi_params)\n","    gradient = flat_concat(grad)\n","    return gradient\n","\n","  def hvp(self, inputs, v_ph):\n","    \"\"\"\n","    hessian vector produce\n","    \"\"\"\n","    pi_params = self.actor.model.trainable_weights\n","    x_ph, a_ph, adv_ph, ret_ph, logp_old_ph, *info_values = inputs\n","\n","    with tf.GradientTape() as tape1:\n","      with tf.GradientTape() as tape0:\n","        pi, logp, log_pi, info, info_phs, d_kl = self.actor.cal_outputs_1(x_ph, a_ph, *info_values)\n","      g = flat_concat(tape0.gradient(d_kl, pi_params))\n","      l = tf.reduce_sum(g * v_ph)\n","    hvp = flat_concat(tape1.gradient(l, pi_params))\n","\n","    if DAMPING_COEFF > 0:\n","      hvp += DAMPING_COEFF * v_ph\n","    return hvp\n","\n","  def get_pi_params(self):\n","    pi_params = self.actor.model.trainable_weights\n","    return flat_concat(pi_params)\n","\n","  def set_pi_params(self, v_ph):\n","    pi_params = self.actor.model.trainable_weights\n","    assign_params_from_flat(v_ph, pi_params)\n","\n","  def save_ckpt(self):\n","    if not os.path.exists('model'):\n","      os.makedirs('model')\n","\n","    tl.files.save_weights_to_hdf5('model/trpo_actor.hdf5', self.actor.model)\n","    tl.files.save_weights_to_hdf5('model/trpo_critic.hdf5', self.critic.model)\n","\n","  def load_ckpt(self):\n","    tl.files.load_hdf5_to_weights_in_order('model/trpo_actor.hdf5', self.actor.model)\n","    tl.files.load_hdf5_to_weights_in_order('model/trpo_critic.hdf5', self.critic.model)\n","\n","  def cg(self, Ax, b):\n","    '''\n","    介于最速下降法与牛顿法之间的一个方法，\n","    它仅需利用一阶导数信息，但克服了最速下降法收敛慢的缺点，\n","    又避免了牛顿法需要存储和计算Hessian矩阵并求逆的缺点\n","    '''\n","    x = np.zeros_like(b)\n","    r = copy.deepcopy(b) # b-Ax(x) 对于x=0 Ax(x)=0\n","    p = copy.deepcopy(r)\n","    r_dot_old = np.dot(r, r)\n","    for _ in range(CG_ITERS):\n","      z = Ax(p)\n","      alpha = r_dot_old / (np.dot(p, z) + EPS)\n","      x += alpha * p\n","      r -= alpha * z\n","      r_dot_new = np.dot(r, r)\n","      p = r + (r_dot_new / r_dot_old) * p\n","      r_dot_old = r_dot_new\n","    return x\n","  \n","  def update(self):\n","    # 准备 hessian func ，gradient eval\n","    inputs = self.buf.get()\n","    Hx = lambda x: self.hvp(inputs, x)\n","    g, pi_l_old, v_l_log = self.gradient(inputs), self.pi_loss(inputs), self.v_loss(inputs)\n","\n","    # 核心\n","    x = self.cg(Hx, g)\n","    alpha = np.sqrt(2 * DELTA / (np.dot(x, Hx(x)) + EPS))\n","    old_params = self.get_pi_params()\n","\n","    def set_and_eval(step):\n","      aa = alpha * x * step\n","      par = old_params - aa\n","      self.set_pi_params(par)\n","      x_ph, a_ph, adv_ph, ret_ph, logp_old_ph, *info_values = inputs\n","      pi, logp, logp_pi, info, info_phs, d_kl = self.actor.cal_outputs_1(x_ph, a_ph, *info_values)\n","      loss = self.pi_loss(inputs)\n","      return [d_kl, loss]\n","\n","    for j in range(BACKTRACK_ITERS):\n","      kl, pi_l_new = set_and_eval(step=BACKTRACK_COEFF**j)\n","      if kl <= DELTA and pi_l_new <= pi_l_old:\n","        # 接受新参数\n","        break\n","\n","      if j == BACKTRACK_ITERS - 1:\n","        # 线性搜索失败，保持旧参数\n","        kl, pi_l_new = set_and_eval(step=0.)\n","\n","    # value func更新\n","    for _ in range(TRAIN_V_ITERS):\n","      self.train_vf(inputs)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iTaFyrYJAf6D","colab_type":"text"},"source":["### **Main （初始化，训练，测试）**"]},{"cell_type":"code","metadata":{"id":"i2P1WZAs-Stq","colab_type":"code","colab":{}},"source":["# 初始化\n","tf.random.set_seed(SEED)\n","np.random.seed(SEED)\n","\n","env = gym.make(ENV_NAME)\n","env.seed(SEED)\n","\n","agent = TRPO(env.observation_space, env.action_space)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SK8e4leVjdy9","colab_type":"code","outputId":"f0e90e04-182e-4578-a413-caabd63c4145","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["start_time = time.time()\n","o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0\n","reward_list = []\n","for epoch in range(EPOCHS):\n","  t0 = time.time()\n","  rew = 0\n","  for t in range(STEPS_PER_EPOCH):\n","    agent_outs = agent.get_action_ops(o.reshape(1, -1))\n","    a, v_t, logp_t, info_t = np.array(agent_outs[0][0], np.float32),\\\n","                   np.array(agent_outs[1], np.float32),\\\n","                   np.array(agent_outs[2], np.float32),\\\n","                   np.array(agent_outs[3:], np.float32)\n","    # if isinstance(env.action_space, Discrete):\n","    #   a = a.astype(np.int32)\n","    agent.buf.store(o, a, r, v_t, logp_t, info_t)\n","\n","    o, r, d, _ = env.step(a)\n","    ep_ret += r\n","    ep_len += 1\n","\n","    terminal = d or (ep_len == MAX_EP_LEN)\n","    if terminal or (t == STEPS_PER_EPOCH - 1):\n","      if not (terminal):\n","        print(\"Warnning: trajectory cut off by epoch at %d steps.\" % ep_len)\n","      last_val = r if d else agent.critic.critic_cal_func(o.reshape(1, -1))\n","      agent.buf.finish_path(last_val)\n","      rew = ep_ret\n","      o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0\n","    \n","  if (epoch % SAVE_FREQ == 0) or (epoch == EPOCHS - 1):\n","    agent.save_ckpt()\n","\n","  agent.update()\n","  print('epoch[{}/{}] ep_ret: {} time: {}'.format(epoch, EPOCHS, rew, time.time() - t0))\n","\n","  reward_list.append(rew)\n","  plt.clf()\n","  plt.ion()\n","  plt.plot(reward_list)\n","  plt.title('TRPO ' + str(DELTA))\n","  plt.ylim(-2000, 0)\n","  plt.show()\n","  plt.pause(0.1)\n","agent.save_ckpt()\n","plt.ioff()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-qDmplgUmcNV","colab_type":"code","colab":{}},"source":["# test\n","agent.load_ckpt()\n","while True:\n","  o = env.reset()\n","  for i in range(STEPS_PER_EPOCH):\n","    # env.render()\n","    agent_outs = agent.get_action_ops(o.reshape(1, -1))\n","    a, v_t, logp_t, info_t = agent_outs[0][0], agent_outs[1], agent_outs[2], agent_outs[3:]\n","    o, r, d, _ = env.step(a)\n","    if d:\n","      break"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KIXvHxpqAy4W","colab_type":"code","outputId":"312c7b80-775e-4244-9f56-369691a55674","executionInfo":{"status":"ok","timestamp":1567680060688,"user_tz":-480,"elapsed":2839,"user":{"displayName":"Steven Iris","photoUrl":"","userId":"04172472830736813201"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["tfp.__version__"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'0.7.0'"]},"metadata":{"tags":[]},"execution_count":49}]}]}