{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TD3_TF.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"hB-PL2kOuMhx","colab_type":"text"},"source":["### **装载云盘**"]},{"cell_type":"code","metadata":{"id":"W3kk56zyuKF8","colab_type":"code","outputId":"d3ce44c9-c513-47a7-b3f3-bdda4d187c6b","executionInfo":{"status":"ok","timestamp":1568266578995,"user_tz":-480,"elapsed":2686,"user":{"displayName":"Steven Iris","photoUrl":"","userId":"04172472830736813201"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ipnfS2Csyu33","colab_type":"text"},"source":["### **安装tensorflow 2.0**"]},{"cell_type":"code","metadata":{"id":"49C0mzzby3s_","colab_type":"code","colab":{}},"source":["!pip install tensorflow-gpu==2.0.0-beta1 "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SOpdHh4dAK0w","colab_type":"code","colab":{}},"source":["# tensorlayer 兼容问题\n","!pip install imgaug==0.2.6"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nDeaUCyG_kbP","colab_type":"code","colab":{}},"source":["!pip install tensorlayer"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wgNs2Gd10f73","colab_type":"text"},"source":["### **cd命令**"]},{"cell_type":"code","metadata":{"id":"BBQU9lh_yls5","colab_type":"code","colab":{}},"source":["import os\n","os.chdir(\"drive/My Drive/RL_EA/Actor_Critic\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3TpgOs8f0lUO","colab_type":"text"},"source":["### **查看当前路径**"]},{"cell_type":"code","metadata":{"id":"c21teJ570bmB","colab_type":"code","outputId":"9f922fa2-a322-48e1-a02b-c3c567d0e4e5","executionInfo":{"status":"ok","timestamp":1568272185512,"user_tz":-480,"elapsed":10419,"user":{"displayName":"Steven Iris","photoUrl":"","userId":"04172472830736813201"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["!pwd"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZJvrmYXhvBQ3","colab_type":"text"},"source":["# Twin Delayed DDPG (TD3)\n","\n","</br>\n","$$\n","off-Policy\\\\\n","continous\n","$$\n","</br>\n","\n","**DDPG学习两个approximator来估计$Q^*(s,a)$，$a^*(s)$。适合continous动作空间。训练的时候加noise能增加前期explore的能力。Actor和Critic均有target网**\n","\n","**TD3 是基于DDPG进行改进的，添加了3个改进点：**\n","\n","\n","**（1）target policy smoothing：**\n","\n","为了防止critic学习过程中进入了错误的“最优”点，对action加了点noise。\n","\n","$$\n","a'(s')=clip(\\mu_{\\theta_{targ}} \\ \\ (s')+clip(\\epsilon,-c,c),a_{Low},a_{High}), \\ \\ \\epsilon\\sim{\\mathcal N}(0,\\sigma)\n","$$\n","\n","**（2）clipped double-Q learning：**\n","\n","用两对critic网来计算Q值，取其中最小的一个进行update。\n","\n","$$\n","y(r,s',d)=r+\\gamma(1-d)\\min_{i=1,2}Q_{\\phi_{i,targ}} \\ \\ (s',a'(s'))\n","$$\n","\n","**（3）delayed update of target and policy networks：**\n","\n","为减少方差，policy更新得比Q慢一步，例如更新critic和actor的频率为2：1。\n","</br>\n","\n","### Critic更新：\n","\n","$$\n","L(\\phi_i,D)=\\underset{(s,a,r,s',d)' \\sim D}{{\\mathrm E}}\\big[(Q_{\\phi_i}(s,a)-y(r,s',d))^2\\big]\n","$$\n","\n","\n","### Actor更新：\n","\n","$$\n","\\max_\\theta \\underset{s \\sim D}{\\mathrm E}\\big[Q_{\\phi_1}(s,\\mu_\\theta(s)) \\big]\n","$$\n","*这里选择1号网络进行计算Q值。"]},{"cell_type":"code","metadata":{"id":"zBschy8n8Ilx","colab_type":"code","colab":{}},"source":["import os\n","import time\n","import random\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from IPython.display import clear_output\n","  \n","import gym\n","import tensorflow as tf\n","import tensorflow_probability as tfp\n","import tensorlayer as tl\n","from tensorlayer.layers import Dense\n","from tensorlayer.models import Model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E2yb3b3UBUbE","colab_type":"code","colab":{}},"source":["tfd = tfp.distributions\n","Normal = tfd.Normal\n","\n","tl.logging.set_verbosity(tl.logging.DEBUG)\n","\n","random.seed(2)\n","np.random.seed(2)\n","tf.random.set_seed(2)\n","\n","##### hyper parameters #####\n","\n","ENV = 'Pendulum-v0'\n","action_range = 1.\n","max_frames = 40000\n","test_frames = 300\n","max_steps = 150\n","batch_size = 64\n","explore_steps = 500\n","update_itr = 3\n","hidden_dim = 32\n","q_lr = 3e-4\n","policy_lr = 3e-4\n","policy_target_update_interval = 3\n","explore_noise_scale = 1.0\n","eval_noise_scale = 0.5\n","reward_scale = 1.\n","replay_buffer_size = 5e5"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VSo35_oXNYmS","colab_type":"code","colab":{}},"source":["# buffer\n","class ReplayBuffer:\n","  def __init__(self, capacity):\n","    self.capacity = capacity\n","    self.buffer = []\n","    self.position = 0\n","  \n","  def push(self, state, action, reward, next_state, done):\n","    if len(self.buffer) < self.capacity:\n","      self.buffer.append(None)\n","    self.buffer[self.position] = (state, action, reward, next_state, done)\n","    self.position = int((self.position + 1) % self.capacity)\n","\n","  def sample(self, batch_size):\n","    batch = random.sample(self.buffer, batch_size)\n","    state, action, reward, next_state, done = map(np.stack, zip(*batch))\n","    '''\n","    * : sum(a,b) <=> batch=(a,b), sum(*batch)\n","    zip : a=[1,2], b=[2,3], zip(a,b) => [(1,2),(2,3)]\n","    map : map(square, [2,3]) => [4,9]\n","    stack : np.stack((1,2)) => array([1,2])\n","    '''\n","    return state, action, reward, next_state, done\n","\n","  def __len__(self):\n","    return len(self.buffer)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HgqjfZ8yPi_c","colab_type":"code","colab":{}},"source":["# utils\n","class NormalizedActions(gym.ActionWrapper):\n","  # action _action\n","  def action(self, action):\n","    low = self.action_space.low\n","    high = self.action_space.high\n","\n","    action = low + (action + 1.0) * 0.5 *(high - low) # ?\n","    action = np.clip(action, low, high)\n","    return action\n","\n","  # reverse_action _reverse_action\n","  def reverse_action(self, action):\n","    low = self.action_space.low\n","    high = self.action_space.high\n","\n","    action = 2 * (action - low) / (high - low) - 1\n","    action = np.clip(action, low, high)\n","    return action\n","  \n","def plot(frame_idx, rewards):\n","  clear_output(True)\n","  plt.figure(figsize=(20,5))\n","  plt.title('frame %s. reward:%s' % (frame_idx, rewards[-1]))\n","  plt.plot(rewards)\n","  plt.xlabel('Episode')\n","  plt.ylabel('Episode Reward')\n","  plt.savefig('td3.png')\n","  # plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VaEZgpBcQh6s","colab_type":"code","colab":{}},"source":["# Network\n","class QNetwork(Model):\n","\n","  def __init__(self, num_inputs, num_actions, hidden_dim, init_w=3e-3):\n","    super(QNetwork, self).__init__()\n","    input_dim = num_inputs + num_actions\n","    w_init = tf.random_uniform_initializer(-init_w, init_w)\n","\n","    self.linear1 = Dense(n_units=hidden_dim, act=tf.nn.relu, W_init=w_init, in_channels=input_dim, name='q1')\n","    self.linear2 = Dense(n_units=hidden_dim, act=tf.nn.relu, W_init=w_init, in_channels=hidden_dim, name='q2')\n","    self.linear3 = Dense(n_units=1, W_init=w_init, in_channels=hidden_dim, name='q3')\n","\n","  def forward(self, input):\n","    x = self.linear1(input)\n","    x = self.linear2(x)\n","    x = self.linear3(x)\n","    return x\n","\n","class PolicyNetwork(Model):\n","\n","  def __init__(self, num_inputs, num_actions, hidden_dim, action_range=1., init_w=3e-3):\n","    super(PolicyNetwork, self).__init__()\n","    w_init = tf.random_uniform_initializer(-init_w, init_w)\n","    self.linear1 = Dense(n_units=hidden_dim, act=tf.nn.relu, W_init=w_init, in_channels=num_inputs, name='policy1')\n","    self.linear2 = Dense(n_units=hidden_dim, act=tf.nn.relu, W_init=w_init, in_channels=hidden_dim, name='policy2')\n","    self.linear3 = Dense(n_units=hidden_dim, act=tf.nn.relu, W_init=w_init, in_channels=hidden_dim, name='policy3')\n","\n","    self.output_linear = Dense(n_units=num_actions, W_init=w_init, \\\n","    b_init=tf.random_uniform_initializer(-init_w, init_w), in_channels=hidden_dim, name='policy_output')\n","    self.action_range = action_range\n","    self.num_actions = num_actions\n","\n","  def forward(self, state):\n","    x = self.linear1(state)\n","    x = self.linear2(x)\n","    x = self.linear3(x)\n","\n","    output = tf.nn.tanh(self.output_linear(x))\n","    return output\n","\n","  def evaluate(self, state, eval_noise_scale):\n","    '''\n","    target policy smooth\n","    '''\n","    state = state.astype(np.float32)\n","    action = self.forward(state)\n","\n","    action = self.action_range * action\n","\n","    # noise\n","    normal = Normal(0, 1)\n","    eval_noise_clip = 2 * eval_noise_scale\n","    noise = normal.sample(action.shape) * eval_noise_scale\n","    noise = tf.clip_by_value(noise, -eval_noise_clip, eval_noise_clip)\n","    action = action + noise\n","\n","    return action\n","\n","  def get_action(self, state, explore_noise_scale):\n","    action = self.forward([state])\n","    action = action.numpy()[0]\n","\n","    # noise\n","    normal = Normal(0, 1)\n","    noise = normal.sample(action.shape) * explore_noise_scale\n","    action = self.action_range * action + noise\n","\n","    return action.numpy()\n","\n","  def sample_action(self, ):\n","    a = tf.random.uniform([self.num_actions], -1, 1)\n","    return self.action_range * a.numpy()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5CLTEDo2UY0S","colab_type":"code","colab":{}},"source":["# TD3\n","class TD3_Trainer:\n","\n","  def __init__(\n","      self, replay_buffer, hidden_dim, action_range, policy_target_update_interval=1, q_lr=3e-4, policy_lr=3e-4\n","  ):\n","    self.replay_buffer = replay_buffer\n","    \n","    self.q_net1 = QNetwork(state_dim, action_dim, hidden_dim)\n","    self.q_net2 = QNetwork(state_dim, action_dim, hidden_dim)\n","    self.target_q_net1 = QNetwork(state_dim, action_dim, hidden_dim)\n","    self.target_q_net2 = QNetwork(state_dim, action_dim, hidden_dim)\n","    self.policy_net = PolicyNetwork(state_dim, action_dim, hidden_dim, action_range)\n","    self.target_policy_net = PolicyNetwork(state_dim, action_dim, hidden_dim, action_range)\n","    print('Q Network (1,2): ',self.q_net1)\n","    print('Policy Network: ',self.policy_net)\n","\n","    self.target_q_net1 = self.target_ini(self.q_net1, self.target_q_net1)\n","    self.target_q_net2 = self.target_ini(self.q_net2, self.target_q_net2)\n","    self.target_policy_net = self.target_ini(self.policy_net, self.target_policy_net)\n","\n","    self.update_cnt = 0\n","    self.policy_target_update_interval = policy_target_update_interval\n","\n","    self.q_optimizer1 = tf.optimizers.Adam(q_lr)\n","    self.q_optimizer2 = tf.optimizers.Adam(q_lr)\n","    self.policy_optimizer = tf.optimizers.Adam(policy_lr)\n","\n","  def target_ini(self, net, target_net):\n","    for target_param, param in zip(target_net.trainable_weights, net.trainable_weights):\n","      target_param.assign(param)\n","    return target_net\n","\n","  def target_soft_update(self, net, target_net, soft_tau):\n","    for target_param, param in zip(target_net.trainable_weights, net.trainable_weights):\n","      target_param.assign(\n","          target_param * (1.0 - soft_tau) + param * soft_tau\n","      )\n","    return target_net\n","\n","  def update(self, batch_size, eval_noise_scale, reward_scale=10., gamma=0.9, soft_tau=1e-2):\n","    self.update_cnt += 1\n","    state, action, reward, next_state, done = self.replay_buffer.sample(batch_size)\n","\n","    reward = reward[:, np.newaxis]\n","    done = done[:, np.newaxis]\n","\n","    new_next_action = self.target_policy_net.evaluate(\n","        next_state, eval_noise_scale=eval_noise_scale\n","    ) # clipped normal noise\n","    # norm reward\n","    reward = reward_scale * (reward - np.mean(reward, axis=0)) / np.std(reward, axis=0)\n","\n","    target_q_input = tf.concat([next_state, new_next_action], 1)\n","    target_q_min = tf.minimum(self.target_q_net1(target_q_input), self.target_q_net2(target_q_input))\n","\n","    target_q_value = reward + (1 - done) * gamma * target_q_min\n","    q_input = tf.concat([state, action], 1)\n","\n","    with tf.GradientTape() as q1_tape:\n","      predicted_q_value1 = self.q_net1(q_input)\n","      q_value_loss1 = tf.reduce_mean(tf.square(predicted_q_value1 - target_q_value))\n","    q1_grad = q1_tape.gradient(q_value_loss1, self.q_net1.trainable_weights)\n","    self.q_optimizer1.apply_gradients(zip(q1_grad, self.q_net1.trainable_weights))\n","\n","    with tf.GradientTape() as q2_tape:\n","      predicted_q_value2 = self.q_net2(q_input)\n","      q_value_loss2 = tf.reduce_mean(tf.square(predicted_q_value2 - target_q_value))\n","    q2_grad = q2_tape.gradient(q_value_loss2, self.q_net2.trainable_weights)\n","    self.q_optimizer2.apply_gradients(zip(q2_grad, self.q_net2.trainable_weights))\n","\n","    # policy\n","    if self.update_cnt % self.policy_target_update_interval == 0:\n","      with tf.GradientTape() as p_tape:\n","        new_action = self.policy_net.evaluate(\n","            state, eval_noise_scale=0.0\n","        ) # 无noise，确定性policy梯度\n","        new_q_input = tf.concat([state, new_action], 1)\n","        predicted_new_q_value = self.q_net1(new_q_input)\n","        policy_loss = -tf.reduce_mean(predicted_new_q_value)\n","        # 另一版本的实现是 Q值为 两个Q网络中最小的\n","      p_grad = p_tape.gradient(policy_loss, self.policy_net.trainable_weights)\n","      self.policy_optimizer.apply_gradients(zip(p_grad, self.policy_net.trainable_weights))\n","\n","      # soft update\n","      self.target_q_net1 = self.target_soft_update(self.q_net1, self.target_q_net1, soft_tau)\n","      self.target_q_net2 = self.target_soft_update(self.q_net2, self.target_q_net2, soft_tau)\n","      self.target_policy_net = self.target_soft_update(self.policy_net, self.target_policy_net, soft_tau)\n","\n","  def save_weights(self):\n","    tl.files.save_npz(self.q_net1.trainable_weights, name='model_q_net1.npz')\n","    tl.files.save_npz(self.q_net2.trainable_weights, name='model_q_net2.npz')\n","    tl.files.save_npz(self.target_q_net1.trainable_weights, name='model_target_q_net1.npz')\n","    tl.files.save_npz(self.target_q_net2.trainable_weights, name='model_target_q_net2.npz')\n","    tl.files.save_npz(self.policy_net.trainable_weights, name='model_policy_net.npz')\n","    tl.files.save_npz(self.target_policy_net.trainable_weights, name='model_target_policy_net.npz')\n","\n","  def load_weights(self):\n","    tl.files.load_and_assign_npz(name='model_q_net1.npz', network=self.q_net1)\n","    tl.files.load_and_assign_npz(name='model_q_net2.npz', network=self.q_net2)\n","    tl.files.load_and_assign_npz(name='model_target_q_net1.npz', network=self.target_q_net1)\n","    tl.files.load_and_assign_npz(name='model_target_q_net2.npz', network=self.target_q_net2)\n","    tl.files.load_and_assign_npz(name='model_policy_net.npz', network=self.policy_net)\n","    tl.files.load_and_assign_npz(name='model_target_policy_net.npz', network=self.target_policy_net)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iTaFyrYJAf6D","colab_type":"text"},"source":["### **Main （初始化，训练，测试）**"]},{"cell_type":"code","metadata":{"id":"DpPdwfHRAVQc","colab_type":"code","cellView":"code","outputId":"e32b2250-ad6f-405c-affc-8e53c722112d","executionInfo":{"status":"ok","timestamp":1568272419545,"user_tz":-480,"elapsed":7340,"user":{"displayName":"Steven Iris","photoUrl":"","userId":"04172472830736813201"}},"colab":{"base_uri":"https://localhost:8080/","height":575}},"source":["# 初始化\n","env = NormalizedActions(gym.make(ENV))\n","action_dim = env.action_space.shape[0]\n","state_dim = env.observation_space.shape[0]\n","replay_buffer = ReplayBuffer(replay_buffer_size)\n","td3_trainer = TD3_Trainer(replay_buffer, hidden_dim=hidden_dim, policy_target_update_interval=policy_target_update_interval,\\\n","action_range=action_range, q_lr=q_lr, policy_lr=policy_lr)\n","td3_trainer.q_net1.train()\n","td3_trainer.q_net2.train()\n","td3_trainer.target_q_net1.train()\n","td3_trainer.target_q_net2.train()\n","td3_trainer.policy_net.train()\n","td3_trainer.target_policy_net.train()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[TL] Dense  q1: 32 relu\n","[TL] Dense  q2: 32 relu\n","[TL] Dense  q3: 1 No Activation\n","[TL] Dense  q1: 32 relu\n","[TL] Dense  q2: 32 relu\n","[TL] Dense  q3: 1 No Activation\n","[TL] Dense  q1: 32 relu\n","[TL] Dense  q2: 32 relu\n","[TL] Dense  q3: 1 No Activation\n","[TL] Dense  q1: 32 relu\n","[TL] Dense  q2: 32 relu\n","[TL] Dense  q3: 1 No Activation\n","[TL] Dense  policy1: 32 relu\n","[TL] Dense  policy2: 32 relu\n","[TL] Dense  policy3: 32 relu\n","[TL] Dense  policy_output: 1 No Activation\n","[TL] Dense  policy1: 32 relu\n","[TL] Dense  policy2: 32 relu\n","[TL] Dense  policy3: 32 relu\n","[TL] Dense  policy_output: 1 No Activation\n","Q Network (1,2):  qnetwork(\n","  (q1): Dense(n_units=32, relu, in_channels='4', name='q1')\n","  (q2): Dense(n_units=32, relu, in_channels='32', name='q2')\n","  (q3): Dense(n_units=1, No Activation, in_channels='32', name='q3')\n",")\n","Policy Network:  policynetwork(\n","  (policy1): Dense(n_units=32, relu, in_channels='3', name='policy1')\n","  (policy2): Dense(n_units=32, relu, in_channels='32', name='policy2')\n","  (policy3): Dense(n_units=32, relu, in_channels='32', name='policy3')\n","  (policy_output): Dense(n_units=1, No Activation, in_channels='32', name='policy_output')\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"33GSLO9ke07S","colab_type":"code","colab":{}},"source":["# train\n","frame_idx = 0\n","rewards = []\n","t0 = time.time()\n","while frame_idx < max_frames:\n","  state = env.reset()\n","  state = state.astype(np.float32)\n","  episode_reward = 0\n","  if frame_idx < 1:\n","    print('intialize')\n","    # extra call 用来使内部func能够使用model.forward\n","    _ = td3_trainer.policy_net([state])\n","    _ = td3_trainer.target_policy_net([state])\n","\n","  for step in range(max_steps):\n","    if frame_idx > explore_steps:\n","      action = td3_trainer.policy_net.get_action(state, explore_noise_scale=1.0)\n","    else:\n","      action = td3_trainer.policy_net.sample_action()\n","\n","    next_state, reward, done, _ = env.step(action)\n","    next_state = next_state.astype(np.float32)\n","    # env.render()\n","    done = 1 if done == True else 0\n","    \n","    replay_buffer.push(state, action, reward, next_state, done)\n","\n","    state = next_state\n","    episode_reward += reward\n","    frame_idx += 1\n","\n","    if len(replay_buffer) > batch_size:\n","      for i in range(update_itr):\n","        td3_trainer.update(batch_size, eval_noise_scale=0.5, reward_scale=1.)\n","    \n","    if frame_idx % 500 == 0:\n","      plot(frame_idx, rewards)\n","    \n","    if done:\n","      break\n","  episode = int(frame_idx / max_steps)  # 当前episode\n","  all_episodes = int(max_frames / max_steps)  # 所有episode\n","  print('Episode:{}/{} | Episode Reward:{:.4f} | Running Time:{:.4f}'\\\n","  .format(episode, all_episodes, episode_reward, time.time() - t0))\n","  rewards.append(episode_reward)\n","td3_trainer.save_weights()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4X20sMl_nSqH","colab_type":"code","outputId":"74c72806-88ab-4e0d-bdc9-752d9ec66ea7","executionInfo":{"status":"ok","timestamp":1568277741547,"user_tz":-480,"elapsed":2225,"user":{"displayName":"Steven Iris","photoUrl":"","userId":"04172472830736813201"}},"colab":{"base_uri":"https://localhost:8080/","height":179}},"source":["# test\n","frame_idx = 0\n","rewards = []\n","t0 = time.time()\n","\n","td3_trainer.load_weights()\n","\n","while frame_idx < test_frames:\n","  state = env.reset()\n","  state = state.astype(np.float32)\n","  episode_reward = 0\n","  if frame_idx < 1:\n","    print('initialize')\n","    _ = td3_trainer.policy_net([state])\n","    _ = td3_trainer.target_policy_net([state])\n","\n","  for step in range(max_steps):\n","    action = td3_trainer.policy_net.get_action(state, explore_noise_scale=1.0)\n","    next_state, reward, done, _ = env.step(action)\n","    next_state = next_state.astype(np.float32)\n","    # env.render()\n","    done = 1 if done == True else 0\n","\n","    state = next_state\n","    episode_reward += reward\n","    frame_idx += 1\n","\n","    if done:\n","      break\n","  episode = int(frame_idx / max_steps)\n","  all_episodes = int(test_frames / max_steps)\n","  print('Episode:{}/{} | Episode Reward:{:.4f} | Running Time:{:.4f}'\\\n","  .format(episode, all_episodes, episode_reward, time.time() - t0))\n","  rewards.append(episode_reward)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[TL] [*] Load model_q_net1.npz SUCCESS!\n","[TL] [*] Load model_q_net2.npz SUCCESS!\n","[TL] [*] Load model_target_q_net1.npz SUCCESS!\n","[TL] [*] Load model_target_q_net2.npz SUCCESS!\n","[TL] [*] Load model_policy_net.npz SUCCESS!\n","[TL] [*] Load model_target_policy_net.npz SUCCESS!\n","initialize\n","Episode:1/2 | Episode Reward:-342.6536 | Running Time:0.6592\n","Episode:2/2 | Episode Reward:-647.6249 | Running Time:1.2619\n"],"name":"stdout"}]}]}