{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SAC_discrete_TF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GaoDongWan/Reinforcement_Learning/blob/master/actor_critic/SAC_discrete_TF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hB-PL2kOuMhx",
        "colab_type": "text"
      },
      "source": [
        "### **装载云盘**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3kk56zyuKF8",
        "colab_type": "code",
        "outputId": "fdeabca1-f397-46da-8ea7-6164880c3244",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipnfS2Csyu33",
        "colab_type": "text"
      },
      "source": [
        "### **安装tensorflow 2.0**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49C0mzzby3s_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0-beta1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOpdHh4dAK0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tensorlayer 兼容问题\n",
        "!pip install imgaug==0.2.6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDeaUCyG_kbP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorlayer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgNs2Gd10f73",
        "colab_type": "text"
      },
      "source": [
        "### **cd命令**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBQU9lh_yls5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir(\"drive/My Drive/RL_EA/Actor_Critic\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TpgOs8f0lUO",
        "colab_type": "text"
      },
      "source": [
        "### **查看当前路径**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c21teJ570bmB",
        "colab_type": "code",
        "outputId": "a8631c8e-53ea-4409-9a3e-0ff504488edd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/RL_EA/Actor_Critic\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJvrmYXhvBQ3",
        "colab_type": "text"
      },
      "source": [
        "# Soft Actor Critic (SAC)\n",
        "\n",
        "</br>\n",
        "$$\n",
        "off-Policy\\\\\n",
        "continous\\ \\&\\ disvrete\n",
        "$$\n",
        "</br>\n",
        "*这里是discrete的代码。按照SAC论文公式调整了一部分代码。\n",
        "\n",
        "</br>\n",
        "\n",
        "</br>\n",
        "\n",
        "**核心思想是通过利用最大熵学习来让策略多元化**\n",
        "\n",
        "\n",
        "<table>\n",
        "<tbody>\n",
        "<tr>\n",
        "<td>\n",
        "<center>\n",
        "<img src=\"https://bair.berkeley.edu/static/blog/softq/figure_3a_unimodal-policy.png\" width=\"80%\" height=\"80%\" />\n",
        "\n",
        "传统RL\n",
        "</center>\n",
        "</td>\n",
        "<td>\n",
        "<center>\n",
        "<img src=\"https://bair.berkeley.edu/static/blog/softq/figure_3b_multimodal_policy.png\" width=\"80%\" height=\"80%\" />\n",
        "\n",
        "多模RL\n",
        "</center>\n",
        "</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>\n",
        "\n",
        "**信息熵定义：**\n",
        "$$\n",
        "H(P)=\\underset{x \\sim P}{\\mathrm E}\\big[-logP(x) \\big]\n",
        "$$\n",
        "\n",
        "**最大熵RL：**\n",
        "$$\n",
        "\\pi^*=\\arg\\max_{\\pi}\\mathbb{E}_{(s_t,a_t)\\sim \\rho_\\pi}[\\sum_{t}{\\underbrace{R(s_t,a_t)}_{reward}} +\\alpha\\underbrace{ H(\\pi(\\cdot|s_t))}_{entropy}]\n",
        "$$\n",
        "</br>\n",
        "*  **SAC中的Q，V：**\n",
        "\n",
        "$$\n",
        "Q(s_t,a_t)=r(s_t,a_t)+\\gamma\\mathbb{E}_{s_{t+1}\\ \\sim\\rho}[V(s_{t+1})]\n",
        "$$\n",
        "</br>\n",
        "$$\n",
        "V(s_t)=\\mathbb{E}_{a_t\\sim\\pi}[Q(s_t,a_t)-\\alpha \\log \\pi(a_t|s_t)]\n",
        "$$\n",
        "</br>\n",
        "$$\n",
        "Q(s_t,a_t)=r(s_t,a_t) + \\gamma\\mathbb{E}_{s_{t+1}\\ \\ ,\\ a_{t+1}}\\ [Q(s_{t+1},a_{t+1})-\\alpha \\log(\\pi(a_{t+1}|s_{t+1}))]\n",
        "$$\n",
        "</br>\n",
        "*  **SAC中的policy：**\n",
        "</br>\n",
        "\n",
        "> 作者认为让$\\pi$服从玻尔兹曼分布能使其达成图2的效果：\n",
        "\n",
        "$$\n",
        "\\pi(a_t|s_t)\\propto exp(-\\mathcal{E}(s_t,a_t))\n",
        "$$\n",
        "\n",
        "> 之后将分布与Q联系起来:\n",
        "\n",
        "$$\n",
        "\\mathcal{E}(s_t,a_t)=-\\frac{1}{\\alpha}Q(s_t,a_t)\n",
        "$$\n",
        "\n",
        "> 得到（$\\alpha$相当于温度系数）：\n",
        "\n",
        "$$\n",
        "\\pi(a_t|s_t)\\propto exp(Q(s_t,a_t))\n",
        "$$\n",
        "\n",
        "> 由此作者提出：\n",
        "\n",
        "$$\n",
        "\\pi'=\\arg \\min_{\\pi_{k}\\ \\in\\Pi}D_{KL}(\\pi_k(\\cdot|s_t)||\\frac{\\exp(\\frac{1}{\\alpha}Q^{\\pi}(s_t,\\cdot))}{Z^{\\pi}(s_t)})\n",
        "$$\n",
        "\n",
        "**Policy更新方式：**\n",
        "\n",
        "$$\n",
        "\\begin{equation} \\begin{split} J_\\pi(\\phi) &= D_\\text{KL} \\big( \\pi_\\phi(. \\vert s_t) \\| \\exp(\\frac{1}{\\alpha}Q_\\theta(s_t, .) - \\log Z(s_t)) \\big) \\\\ &= \\mathbb{E}_{s_t\\sim\\mathcal{D},a_t\\sim \\pi_\\phi} \\Big[ \\log \\big( \\frac{\\pi_\\phi(a_t \\vert s_t)}{\\exp(\\frac{1}{\\alpha}Q_\\theta(s_t, a_t) - \\log Z(s_t))} \\big) \\Big] \\\\\\ &= \\mathbb{E}_{s_t\\sim\\mathcal{D},a_t\\sim \\pi_\\phi} [ \\log \\pi_\\phi(a_t \\vert s_t) -\\frac{1}{\\alpha} Q_\\theta(s_t, a_t) + \\log Z(s_t) ] \\end{split} \\end{equation}\n",
        "$$\n",
        "\n",
        "这里的$Z$是配分函数，在计算梯度中没有其它作用，可以忽略掉。\n",
        "\n",
        "**动作选择：**\n",
        "\n",
        "由于直接sample不能计算梯度，因此用到了reparameterization trick。\n",
        "\n",
        "$$\\mathbf{u}_t =\\mu_t + \\varepsilon_t \\odot \\sigma_t$$\n",
        "\n",
        "$$a_t = \\tanh (\\mathbf{u})$$\n",
        "\n",
        "此外，由于输出的continous的act值域太大，而具体的env的act维度往往在一个范围内，因此用tanh对act进行约束，这样又改变了分布，影响log似然的计算，作者给出了一个计算方式：\n",
        "\n",
        "$$\n",
        "\\log \\pi(a|s)=\\log \\mu(\\mathbf{u}|s)-\\sum_{i=1}^{D}{\\log(1-\\tanh^2(u_i))}\n",
        "$$\n",
        "\n",
        "**第一个版本的SAC除了Q和policy外，还用V来提高训练稳定性；而新版本（本文）的SAC直接用Q，policy，并加上了自动调整的$\\alpha$。**\n",
        "\n",
        "自适应α就是将最大化reward的问题加了一个约束：\n",
        "\n",
        "$$\n",
        "\\max_{\\pi_0, \\dots, \\pi_T} \\mathbb{E} \\Big[ \\sum_{t=0}^T r(s_t, a_t)\\Big] \\text{s.t. } \\forall t\\text{, } \\mathcal{H}(\\pi_t) \\geq \\mathcal{H}_0\n",
        "$$\n",
        "\n",
        "再通过拉格朗日对偶的思想，得到阿尔法的更新方式：\n",
        "\n",
        "$$\n",
        "J(\\alpha) = \\mathbb{E}_{a_t \\sim \\pi_t} [-\\alpha \\log \\pi_t(a_t\\mid\\pi_t) - \\alpha \\mathcal{H}_0]\n",
        "$$\n",
        "\n",
        "原文设置$\\mathcal{H}_0=-\\dim(\\mathcal{A})$\n",
        "\n",
        "**总结一下新版SAC的学习方式（Q：$\\theta$，$\\pi$：$\\phi$，$\\alpha$）：**\n",
        "\n",
        "1.   Q（类似TD3，用到了两组Q网络，选min）：\n",
        "\n",
        "$$\n",
        "Q'(s_t,a_t)=r(s_t,a_t) + \\gamma\\mathbb{E}_{s_{t+1}\\ \\ ,\\ a_{t+1}}\\ \\big[\\min_{i}Q_{\\theta_{targ,i}}\\ \\ (s_{t+1},a_{t+1})-\\alpha \\log(\\pi_{\\phi}(a_{t+1}|s_{t+1}))\\big]\n",
        "$$\n",
        "\n",
        "$$\n",
        " \\begin{split} J_Q(\\theta)&=\\mathbb{E}_{(s_t,a_t,s_{t+1}\\ )\\sim \\mathcal{D},a_{t+1}\\ \\sim\\pi_{\\phi}}[\\frac{1}{2}(Q_{\\theta_i}(s_t,a_t) - Q'(s_t,a_t))^2]  \\end{split} \n",
        "$$\n",
        "\n",
        "2.   action选择（$\\mu_t，\\sigma_t$由actor得到，$\\varepsilon_t$由固定的分布[比如高斯]随机采样得到）,这份代码在log$\\pi$后面加了个正则项：\n",
        "\n",
        "$$\\mathbf{u}_t =\\mu_t + \\varepsilon_t \\odot \\sigma_t$$\n",
        "\n",
        "$$a_t = \\tanh (\\mathbf{u})$$\n",
        "\n",
        "$$\n",
        "\\log \\pi(a|s)=\\log \\mu(\\mathbf{u}|s)-\\sum_{i=1}^{D}{\\log(1-\\tanh^2(u_i))}\n",
        "$$\n",
        "\n",
        "3.   Policy：\n",
        "\n",
        "$$\n",
        "J_\\pi(\\phi)=\\mathbb{E}_{s_t\\sim\\mathcal{D},a_t\\sim \\pi_\\phi} [\\alpha \\log \\pi_\\phi(a_t \\vert s_t) - Q_\\theta(s_t, a_t) ]\n",
        "$$\n",
        "\n",
        "4.   $\\alpha$：\n",
        "\n",
        "$$\n",
        "J(\\alpha) = -\\mathbb{E}_{a_t \\sim \\pi_t} [\\alpha (\\log \\pi_t(a_t\\mid\\pi_t) +  \\mathcal{H}_0)]\n",
        "$$\n",
        "\n",
        "**原文及代码使用的高斯分布去近似玻尔兹曼分布，所以实际上SAC并不是像图2那样是严格多模的。**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBschy8n8Ilx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import time\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import gym\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import tensorlayer as tl\n",
        "from tensorlayer.layers import Dense\n",
        "from tensorlayer.models import Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2yb3b3UBUbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfp = tfp.distributions\n",
        "Normal = tfp.Normal\n",
        "Categorical = tfp.Categorical\n",
        "tl.logging.set_verbosity(tl.logging.DEBUG)\n",
        "\n",
        "random.seed(11)\n",
        "np.random.seed(11)\n",
        "tf.random.set_seed(11)\n",
        "\n",
        "##### hyper parameters #####\n",
        "\n",
        "ENV = 'CartPole-v0'\n",
        "action_range = 1.\n",
        "max_frames = 601\n",
        "test_frames = 30\n",
        "max_steps = 150\n",
        "batch_size = 64\n",
        "explore_steps = 50\n",
        "update_itr = 1\n",
        "hidden_dim = 32\n",
        "soft_q_lr = 3e-4\n",
        "policy_lr = 3e-4\n",
        "alpha_lr = 3e-4\n",
        "reward_scale = 1.\n",
        "replay_buffer_size = 10000\n",
        "\n",
        "AUTO_ENTROPY = True\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgqjfZ8yPi_c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# buffer\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(None)\n",
        "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
        "        self.position = int((self.position + 1) % self.capacity)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
        "        '''\n",
        "        * : sum(a,b) <=> batch=(a,b), sum(*batch)\n",
        "        zip : a=[1,2], b=[2,3], zip(a,b) => [(1,2),(2,3)]\n",
        "        map : map(square, [2,3]) => [4,9]\n",
        "        stack : np.stack((1,2)) => array([1,2])\n",
        "        '''\n",
        "        return state, action, reward, next_state, done\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1kAPYxkDvfm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# utils\n",
        "def plot(frame_idx, rewards):\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.title('frame %s. reward:%s' % (frame_idx, rewards[-1]))\n",
        "    plt.plot(rewards)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Episode Reward')\n",
        "    plt.savefig('sac.png')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaEZgpBcQh6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Network\n",
        "class SoftQNetwork(Model):\n",
        "\n",
        "    def __init__(self, num_inputs, hidden_dim):\n",
        "        super(SoftQNetwork, self).__init__()\n",
        "        input_dim = num_inputs\n",
        "        w_init = tf.keras.initializers.glorot_normal(seed=None)\n",
        "\n",
        "        self.linear1 = Dense(n_units=hidden_dim, act=tf.nn.relu, W_init=w_init, in_channels=input_dim, name='q1')\n",
        "        self.linear2 = Dense(n_units=hidden_dim, act=tf.nn.relu, W_init=w_init, in_channels=hidden_dim, name='q2')\n",
        "        self.linear3 = Dense(n_units=2, W_init=w_init, in_channels=hidden_dim, name='q3')\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.linear1(input)\n",
        "        x = self.linear2(x)\n",
        "        x = self.linear3(x)\n",
        "        return x\n",
        "\n",
        "class PolicyNetwork(Model):\n",
        "\n",
        "    def __init__(self, num_inputs, hidden_dim, init_w=3e-3):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        w_init = tf.random_uniform_initializer(-init_w, init_w)\n",
        "        self.linear1 = Dense(n_units=hidden_dim, act=tf.nn.relu, W_init=w_init, in_channels=num_inputs,\n",
        "                             name='policy1')\n",
        "        self.linear2 = Dense(n_units=hidden_dim, act=tf.nn.relu, W_init=w_init, in_channels=hidden_dim,\n",
        "                             name='policy2')\n",
        "        self.linear3 = Dense(n_units=hidden_dim, act=tf.nn.relu, W_init=w_init, in_channels=hidden_dim,\n",
        "                             name='policy3')\n",
        "\n",
        "        self.act_prob = Dense(2, act=tf.nn.softmax, W_init=w_init,\n",
        "                              b_init=tf.random_normal_initializer(-init_w, init_w), in_channels=hidden_dim,\n",
        "                              name='act_prob')\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = self.linear1(state)\n",
        "        x = self.linear2(x)\n",
        "        x = self.linear3(x)\n",
        "\n",
        "        act_prob = self.act_prob(x)\n",
        "\n",
        "        return act_prob\n",
        "\n",
        "    def evaluate(self, state):\n",
        "        state = state.astype(np.float32)\n",
        "        action_prob = self.forward(state)\n",
        "        action_distribution = Categorical(logits=action_prob)\n",
        "        action = action_distribution.sample()\n",
        "        z = action_prob == 0.0\n",
        "        z = np.array(z).astype(np.float32) * 1e-8\n",
        "        log_act_prob = tf.math.log(action_prob + z)\n",
        "\n",
        "        return action, action_prob, log_act_prob, z\n",
        "\n",
        "    def get_action(self, state):\n",
        "        act_prob = self.forward([state])\n",
        "        max_prob_act = tf.argmax(act_prob, axis=1)\n",
        "        return np.array(max_prob_act[0])\n",
        "\n",
        "    def sample_action(self, ):\n",
        "        a = random.randint(0, 1)\n",
        "\n",
        "        return np.array(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CLTEDo2UY0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SAC\n",
        "class SAC_Trainer:\n",
        "\n",
        "    def __init__(\n",
        "            self, replay_buffer, hidden_dim, soft_q_lr=3e-4, policy_lr=3e-4, alpha_lr=3e-4\n",
        "    ):\n",
        "        self.replay_buffer = replay_buffer\n",
        "\n",
        "        self.soft_q_net1 = SoftQNetwork(state_dim, hidden_dim)\n",
        "        self.soft_q_net2 = SoftQNetwork(state_dim, hidden_dim)\n",
        "        self.target_soft_q_net1 = SoftQNetwork(state_dim, hidden_dim)\n",
        "        self.target_soft_q_net2 = SoftQNetwork(state_dim, hidden_dim)\n",
        "        self.policy_net = PolicyNetwork(state_dim, hidden_dim)\n",
        "        self.log_alpha = tf.Variable(0, dtype=np.float32, name='log_alpha')\n",
        "        self.alpha = tf.math.exp(self.log_alpha)\n",
        "        print('Q Network (1,2): ', self.soft_q_net1)\n",
        "        print('Policy Network: ', self.policy_net)\n",
        "\n",
        "        self.target_soft_q_net1 = self.target_ini(self.soft_q_net1, self.target_soft_q_net1)\n",
        "        self.target_soft_q_net2 = self.target_ini(self.soft_q_net2, self.target_soft_q_net2)\n",
        "\n",
        "        self.soft_q_optimizer1 = tf.optimizers.Adam(soft_q_lr)\n",
        "        self.soft_q_optimizer2 = tf.optimizers.Adam(soft_q_lr)\n",
        "        self.policy_optimizer = tf.optimizers.Adam(policy_lr)\n",
        "        self.alpha_optimizer = tf.optimizers.Adam(alpha_lr)\n",
        "\n",
        "\n",
        "\n",
        "    def target_ini(self, net, target_net):\n",
        "        for target_param, param in zip(target_net.trainable_weights, net.trainable_weights):\n",
        "            target_param.assign(param)\n",
        "        return target_net\n",
        "\n",
        "    def target_soft_update(self, net, target_net, soft_tau):\n",
        "        for target_param, param in zip(target_net.trainable_weights, net.trainable_weights):\n",
        "            target_param.assign(\n",
        "                target_param * (1.0 - soft_tau) + param * soft_tau\n",
        "            )\n",
        "        return target_net\n",
        "\n",
        "    def update(self, batch_size, auto_entropy=True, target_entropy=-2, gamma=0.99, soft_tau=1e-2):\n",
        "        state, action, reward, next_state, done = self.replay_buffer.sample(batch_size)\n",
        "\n",
        "        action = action[:, np.newaxis]\n",
        "        reward = reward[:, np.newaxis]\n",
        "        done = done[:, np.newaxis]\n",
        "\n",
        "        # reward = reward_scale * (reward - np.mean(reward, axis=0)) / np.std(reward, axis=0)\n",
        "\n",
        "        # Q func\n",
        "        new_next_action, next_act_prob, next_log_prob, _ = self.policy_net.evaluate(next_state)\n",
        "        target_q_input = next_state\n",
        "        target_q_min = tf.minimum(self.target_soft_q_net1(target_q_input), self.target_soft_q_net2(target_q_input)) \\\n",
        "                       - self.alpha * next_log_prob\n",
        "        # TODO mean是干什么的，不应该是sum吗？ 是sum了 ，只不过多除了个2。\n",
        "        target_q_min = tf.reduce_mean(next_act_prob * target_q_min, axis=1, keepdims=True)\n",
        "        target_q_value = reward + (1 - done) * gamma * target_q_min\n",
        "        q_input = state\n",
        "\n",
        "        with tf.GradientTape() as q1_tape:\n",
        "            predicted_q_value1 = self.soft_q_net1(q_input)\n",
        "            predicted_q_value1 = tf.reduce_sum(tf.squeeze(tf.one_hot(action, 2)) * predicted_q_value1, axis=1,\n",
        "                                               keepdims=True)\n",
        "            q_value_loss1 = tf.reduce_mean(tf.losses.mean_squared_error(predicted_q_value1, target_q_value))\n",
        "        q1_grad = q1_tape.gradient(q_value_loss1, self.soft_q_net1.trainable_weights)\n",
        "        self.soft_q_optimizer1.apply_gradients(zip(q1_grad, self.soft_q_net1.trainable_weights))\n",
        "\n",
        "        with tf.GradientTape() as q2_tape:\n",
        "            predicted_q_value2 = self.soft_q_net2(q_input)\n",
        "            predicted_q_value2 = tf.reduce_sum(tf.squeeze(tf.one_hot(action, 2)) * predicted_q_value2, axis=1,\n",
        "                                               keepdims=True)\n",
        "            q_value_loss2 = tf.reduce_mean(tf.losses.mean_squared_error(predicted_q_value2, target_q_value))\n",
        "        q2_grad = q2_tape.gradient(q_value_loss2, self.soft_q_net2.trainable_weights)\n",
        "        self.soft_q_optimizer2.apply_gradients(zip(q2_grad, self.soft_q_net2.trainable_weights))\n",
        "\n",
        "        # policy\n",
        "        with tf.GradientTape() as p_tape:\n",
        "            action, act_prob, log_prob, _ = self.policy_net.evaluate(state)\n",
        "            q_input = state\n",
        "            predicted_new_q_value = tf.minimum(self.soft_q_net1(q_input), self.soft_q_net2(q_input))\n",
        "            policy_loss = tf.reduce_mean(act_prob * (self.alpha * log_prob - predicted_new_q_value))\n",
        "        p_grad = p_tape.gradient(policy_loss, self.policy_net.trainable_weights)\n",
        "        self.policy_optimizer.apply_gradients(zip(p_grad, self.policy_net.trainable_weights))\n",
        "\n",
        "        log_prob = tf.reduce_sum(act_prob * log_prob, axis=1)\n",
        "        # alpha w.r.t entropy\n",
        "        # trade_off entropy and maxQ\n",
        "        if auto_entropy is True:\n",
        "            with tf.GradientTape() as alpha_tape:\n",
        "                alpha_loss = -tf.reduce_mean((self.log_alpha * (log_prob + target_entropy)))\n",
        "            alpha_grad = alpha_tape.gradient(alpha_loss, [self.log_alpha])\n",
        "            self.alpha_optimizer.apply_gradients(zip(alpha_grad, [self.log_alpha]))\n",
        "            self.alpha = tf.math.exp(self.log_alpha)\n",
        "        else:\n",
        "            self.alpha = 1.\n",
        "            alpha_loss = 0\n",
        "\n",
        "        # soft update\n",
        "        self.target_soft_q_net1 = self.target_soft_update(self.soft_q_net1, self.target_soft_q_net1, soft_tau)\n",
        "        self.target_soft_q_net2 = self.target_soft_update(self.soft_q_net2, self.target_soft_q_net2, soft_tau)\n",
        "\n",
        "    def save_weights(self):\n",
        "        tl.files.save_npz(self.soft_q_net1.trainable_weights, name='sac_d/model_q_net1.npz')\n",
        "        tl.files.save_npz(self.soft_q_net2.trainable_weights, name='sac_d/model_q_net2.npz')\n",
        "        tl.files.save_npz(self.target_soft_q_net1.trainable_weights, name='sac_d/model_target_q_net1.npz')\n",
        "        tl.files.save_npz(self.target_soft_q_net2.trainable_weights, name='sac_d/model_target_q_net2.npz')\n",
        "        tl.files.save_npz(self.policy_net.trainable_weights, name='sac_d/model_policy_net.npz')\n",
        "\n",
        "    def load_weights(self):\n",
        "        tl.files.load_and_assign_npz(name='sac_d/model_q_net1.npz', network=self.soft_q_net1)\n",
        "        tl.files.load_and_assign_npz(name='sac_d/model_q_net2.npz', network=self.soft_q_net2)\n",
        "        tl.files.load_and_assign_npz(name='sac_d/model_target_q_net1.npz', network=self.target_soft_q_net1)\n",
        "        tl.files.load_and_assign_npz(name='sac_d/model_target_q_net2.npz', network=self.target_soft_q_net2)\n",
        "        tl.files.load_and_assign_npz(name='sac_d/model_policy_net.npz', network=self.policy_net)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTaFyrYJAf6D",
        "colab_type": "text"
      },
      "source": [
        "### **Main （初始化，训练，测试）**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpPdwfHRAVQc",
        "colab_type": "code",
        "cellView": "code",
        "outputId": "cb2f10fc-c39c-419f-f324-3e563a06c7c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "source": [
        "# 初始化\n",
        "env = gym.make(ENV)\n",
        "action_dim = env.action_space.n\n",
        "state_dim = env.observation_space.shape[0]\n",
        "replay_buffer = ReplayBuffer(replay_buffer_size)\n",
        "sac_trainer = SAC_Trainer(replay_buffer, hidden_dim=hidden_dim, soft_q_lr=soft_q_lr, \n",
        "                          policy_lr=policy_lr, alpha_lr=alpha_lr)\n",
        "sac_trainer.soft_q_net1.train()\n",
        "sac_trainer.soft_q_net2.train()\n",
        "sac_trainer.target_soft_q_net1.infer()\n",
        "sac_trainer.target_soft_q_net2.infer()\n",
        "sac_trainer.policy_net.train()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[TL] Dense  q1: 32 relu\n",
            "[TL] Dense  q2: 32 relu\n",
            "[TL] Dense  q3: 2 No Activation\n",
            "[TL] Dense  q1: 32 relu\n",
            "[TL] Dense  q2: 32 relu\n",
            "[TL] Dense  q3: 2 No Activation\n",
            "[TL] Dense  q1: 32 relu\n",
            "[TL] Dense  q2: 32 relu\n",
            "[TL] Dense  q3: 2 No Activation\n",
            "[TL] Dense  q1: 32 relu\n",
            "[TL] Dense  q2: 32 relu\n",
            "[TL] Dense  q3: 2 No Activation\n",
            "[TL] Dense  policy1: 32 relu\n",
            "[TL] Dense  policy2: 32 relu\n",
            "[TL] Dense  policy3: 32 relu\n",
            "[TL] Dense  act_prob: 2 softmax_v2\n",
            "Q Network (1,2):  softqnetwork(\n",
            "  (q1): Dense(n_units=32, relu, in_channels='4', name='q1')\n",
            "  (q2): Dense(n_units=32, relu, in_channels='32', name='q2')\n",
            "  (q3): Dense(n_units=2, No Activation, in_channels='32', name='q3')\n",
            ")\n",
            "Policy Network:  policynetwork(\n",
            "  (act_prob): Dense(n_units=2, softmax_v2, in_channels='32', name='act_prob')\n",
            "  (policy1): Dense(n_units=32, relu, in_channels='4', name='policy1')\n",
            "  (policy2): Dense(n_units=32, relu, in_channels='32', name='policy2')\n",
            "  (policy3): Dense(n_units=32, relu, in_channels='32', name='policy3')\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33GSLO9ke07S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train\n",
        "frame_idx = 0\n",
        "rewards = []\n",
        "t0 = time.time()\n",
        "while frame_idx < max_frames:\n",
        "    state = env.reset()\n",
        "    state = state.astype(np.float32)\n",
        "    episode_reward = 0\n",
        "    if frame_idx < 1:\n",
        "        print('intialize')\n",
        "        # extra call 用来使内部func能够使用model.forward\n",
        "        _ = sac_trainer.policy_net([state])\n",
        "\n",
        "\n",
        "    while True:\n",
        "        if frame_idx > explore_steps:\n",
        "            action = sac_trainer.policy_net.get_action(state)\n",
        "        else:\n",
        "            action = sac_trainer.policy_net.sample_action()\n",
        "\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        next_state = next_state.astype(np.float32)\n",
        "        # env.render()\n",
        "        done = 1 if done == True else 0\n",
        "\n",
        "        replay_buffer.push(state, action, reward, next_state, done)\n",
        "\n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "\n",
        "\n",
        "        if len(replay_buffer) > batch_size:\n",
        "            for i in range(update_itr):\n",
        "                sac_trainer.update(batch_size, auto_entropy=AUTO_ENTROPY, \n",
        "                                   target_entropy=-np.log((1.0 / action_dim)) * 0.98)\n",
        "        if done:\n",
        "            break\n",
        "    # episode = int(frame_idx / max_steps)  # 当前episode\n",
        "    # all_episodes = int(max_frames / max_steps)  # 所有episode\n",
        "    print('Episode:{}/{} | Episode Reward:{:.4f} | Running Time:{:.4f}' \\\n",
        "          .format(frame_idx, max_frames, episode_reward, time.time() - t0))\n",
        "    rewards.append(episode_reward)\n",
        "    if frame_idx % 100 == 0:\n",
        "        plot(frame_idx, rewards)\n",
        "    frame_idx += 1\n",
        "sac_trainer.save_weights()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4X20sMl_nSqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test\n",
        "frame_idx = 0\n",
        "rewards = []\n",
        "t0 = time.time()\n",
        "sac_trainer.load_weights()\n",
        "while frame_idx < test_frames:\n",
        "    state = env.reset()\n",
        "    state = state.astype(np.float32)\n",
        "    episode_reward = 0\n",
        "    if frame_idx < 1:\n",
        "        print('intialize')\n",
        "        # extra call 用来使内部func能够使用model.forward\n",
        "        _ = sac_trainer.policy_net([state])\n",
        "\n",
        "    while True:\n",
        "\n",
        "        action = sac_trainer.policy_net.get_action(state)\n",
        "\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        next_state = next_state.astype(np.float32)\n",
        "        # env.render()\n",
        "        done = 1 if done == True else 0\n",
        "\n",
        "\n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    print('Episode:{}/{} | Episode Reward:{:.4f} | Running Time:{:.4f}' \\\n",
        "          .format(frame_idx, test_frames, episode_reward, time.time() - t0))\n",
        "    rewards.append(episode_reward)\n",
        "    frame_idx += 1"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
