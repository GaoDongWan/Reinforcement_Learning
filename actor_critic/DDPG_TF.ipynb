{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DDPG_TF.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"hB-PL2kOuMhx","colab_type":"text"},"source":["### **装载云盘**"]},{"cell_type":"code","metadata":{"id":"W3kk56zyuKF8","colab_type":"code","outputId":"a1448122-6c32-409e-cf59-17264901385a","executionInfo":{"status":"ok","timestamp":1567496325760,"user_tz":-480,"elapsed":39399,"user":{"displayName":"Steven Iris","photoUrl":"","userId":"04172472830736813201"}},"colab":{"base_uri":"https://localhost:8080/","height":127}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ipnfS2Csyu33","colab_type":"text"},"source":["### **安装tensorflow 2.0**"]},{"cell_type":"code","metadata":{"id":"49C0mzzby3s_","colab_type":"code","colab":{}},"source":["!pip install tensorflow-gpu==2.0.0-beta1 "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SOpdHh4dAK0w","colab_type":"code","colab":{}},"source":["# tensorlayer 兼容问题\n","!pip install imgaug==0.2.6"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nDeaUCyG_kbP","colab_type":"code","colab":{}},"source":["!pip install tensorlayer"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wgNs2Gd10f73","colab_type":"text"},"source":["### **cd命令**"]},{"cell_type":"code","metadata":{"id":"BBQU9lh_yls5","colab_type":"code","colab":{}},"source":["import os\n","os.chdir(\"drive/My Drive/RL_EA/Actor_Critic\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3TpgOs8f0lUO","colab_type":"text"},"source":["### **查看当前路径**"]},{"cell_type":"code","metadata":{"id":"c21teJ570bmB","colab_type":"code","outputId":"91af550d-9d4b-4b9e-ad0c-38406553e85e","executionInfo":{"status":"ok","timestamp":1567516962501,"user_tz":-480,"elapsed":3596,"user":{"displayName":"Steven Iris","photoUrl":"","userId":"04172472830736813201"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["!pwd"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/RL_EA/Actor_Critic\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZJvrmYXhvBQ3","colab_type":"text"},"source":["# Deep Deterministic Policy Gradient\n","\n","</br>\n","$$\n","off-Policy\\\\\n","continous\n","$$\n","</br>\n","\n","**DDPG学习两个approximator来估计$Q^*(s,a)$，$a^*(s)$。适合continous动作空间。训练的时候加noise能增加前期explore的能力。Actor和Critic均有target网**\n","\n","### Q-function相关核心公式：\n","\n","</br>\n","\n","$Bellmen Optimal Equation$：\n","$$\n","Q^*(s,a) = \\underset{s' \\sim P}{{\\mathrm E}}\\left[r(s,a) + \\gamma \\max_{a'} Q^*(s', a')\\right]\n","$$\n","\n","</br>\n","\n","用一个网络来近似最优$Q-function$：\n","\n","</br>\n","\n","&emsp;&emsp;一般情况：\n","\n","$$\n","L(\\phi,D)=\\underset{(s,a,r,s',d)' \\sim P}{{\\mathrm E}}\\big[(Q_\\phi(s,a)-(r+\\gamma(1-d)\\max_{a'}Q_\\phi(s',a')))^2\\big]\n","$$\n","\n","</br>\n","\n","&emsp;&emsp;DDPG(由于连续空间中不能很容易地罗列出所有的状态动作值，**因此对max操作进行了调整**)：\n","\n","$$\n","L(\\phi,D)=\\underset{(s,a,r,s',d)' \\sim P}{{\\mathrm E}}\\big[(Q_\\phi(s,a)-(r+\\gamma(1-d)\\max_{a'}Q_{\\phi_{targ}}\\ (s',\\mu_{\\theta_{targ}}\\ (s'))))^2\\big]\n","$$\n","\n","</br>\n","\n","利用$Experience\\ Replay\\ Buffer$和$Target$网，target网络参数更新方式：\n","\n","$$\n","\\phi_{\\text{targ}} \\leftarrow \\rho \\phi_{\\text{targ}} + (1 - \\rho) \\phi\n","$$\n","\n","</br>\n","\n","### Policy相关核心公式：\n","\n","</br>\n","\n","学的是一个确定性的策略$\\mu_\\theta(s)$，其选择的动作能最大化$Q_\\phi(s,a)$，这里假设$Q-function$是针对动作可微的，那么$Policy$的学习通过下面公式来解决(将$Q-function$相关的参数当常量计算)：\n","\n","\n","$$\n","\\max_\\theta \\underset{s \\sim D}{\\mathrm E}\\big[Q_\\phi(s,\\mu_\\theta(s)) \\big]\n","$$"]},{"cell_type":"code","metadata":{"id":"zBschy8n8Ilx","colab_type":"code","colab":{}},"source":["import os\n","import time\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","  \n","import gym\n","import tensorflow as tf\n","import tensorlayer as tl"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E2yb3b3UBUbE","colab_type":"code","colab":{}},"source":["# tl.logging.set_verbosity(tl.logging.DEBUG)\n","\n","# np.random.seed(3)\n","# tf.random.set_seed(3)\n","\n","##### hyper parameters #####\n","\n","ENV_NAME = 'Pendulum-v0'\n","RANDOMSEED = 1\n","\n","LR_A = 0.001 # actor lr\n","LR_C = 0.002 # critic lr\n","GAMMA = 0.9\n","TAU = 0.01 # soft replace (target网络更新)\n","MEMORY_CAPACITY = 10000\n","BATCH_SIZE = 32\n","\n","MAX_EPISODES = 200 # 训练episode数\n","MAX_EP_STEPS = 200 # 每个episode的step(步)数\n","TEST_PER_EPISODES = 10 # 每episodes测试模型？？\n","VAR = 3 # 控制explore"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xnSyfT13BzfE","colab_type":"code","colab":{}},"source":["##### DDPG #####\n","class DDPG:\n","\n","  def __init__(self, a_dim, s_dim, a_bound):\n","    # (s,a,r,s')\n","    self.memory = np.zeros((MEMORY_CAPACITY, s_dim * 2 + a_dim + 1), dtype=np.float32)\n","    self.pointer = 0\n","    self.a_dim, self.s_dim, self.a_bound = a_dim, s_dim, a_bound\n","\n","    W_init = tf.random_normal_initializer(mean=0, stddev=0.3)\n","    b_init = tf.constant_initializer(0.1)\n","\n","    def get_actor(input_state_shape, name=''):\n","      \"\"\"\n","      return: act\n","      \"\"\"\n","      inputs = tl.layers.Input(input_state_shape, name='A_input')\n","      x = tl.layers.Dense(n_units=30, act=tf.nn.relu, W_init=W_init, b_init=b_init, name='A_l1')(inputs)\n","      x = tl.layers.Dense(n_units=a_dim, act=tf.nn.tanh, W_init=W_init, b_init=b_init, name='A_a')(x)\n","      # lambda 层\n","      x = tl.layers.Lambda(lambda x: np.array(a_bound) * x)(x)\n","      return tl.models.Model(inputs=inputs, outputs=x, name='Actor' + name)\n","\n","    def get_critic(input_state_shape, input_action_shape, name=''):\n","      \"\"\"\n","      return: Q(s,a)\n","      \"\"\"\n","      s = tl.layers.Input(input_state_shape, name='C_s_input')\n","      a = tl.layers.Input(input_action_shape, name='C_a_input')\n","      # 将s，a均作为输入，之前的PG，DQN等都是只把s当作输入，输出一个向量表示Q(s,a)。\n","      x = tl.layers.Concat(1)([s, a])\n","      x = tl.layers.Dense(n_units=60, act=tf.nn.relu, W_init=W_init, b_init=b_init, name='C_l1')(x)\n","      x = tl.layers.Dense(n_units=1, W_init=W_init, b_init=b_init, name='C_out')(x)\n","      return tl.models.Model(inputs=[s, a], outputs=x, name='Critic' + name)\n","\n","    self.actor = get_actor([None, s_dim])\n","    self.critic = get_critic([None, s_dim], [None, a_dim])\n","    self.actor.train()\n","    self.critic.train()\n","\n","    def copy_para(from_model, to_model):\n","      \"\"\"\n","      from_model: 最新模型\n","      to_model: target模型\n","      初始化用的硬更新\n","      \"\"\"\n","      for i, j in zip(from_model.trainable_weights, to_model.trainable_weights):\n","        j.assign(i)\n","\n","    self.actor_target = get_actor([None, s_dim], name='_target')\n","    copy_para(self.actor, self.actor_target)\n","    self.actor_target.eval()\n","\n","    self.critic_target = get_critic([None, s_dim], [None, a_dim], name='_target')\n","    copy_para(self.critic, self.critic_target)\n","    self.critic_target.eval()\n","\n","    self.R = tl.layers.Input([None, 1], tf.float32, 'r')\n","    # soft replacement\n","    self.ema = tf.train.ExponentialMovingAverage(decay=1-TAU)\n","\n","    self.actor_opt = tf.optimizers.Adam(LR_A)\n","    self.critic_opt = tf.optimizers.Adam(LR_C)\n","\n","  def ema_update(self):\n","    \"\"\"\n","    利用滑动平均实现软更新\n","    \"\"\"\n","    paras = self.actor.trainable_weights + self.critic.trainable_weights\n","    self.ema.apply(paras)\n","    for i, j in zip(self.actor_target.trainable_weights + self.critic_target.trainable_weights, paras):\n","      i.assign(self.ema.average(j))  \n","\n","  def choose_action(self, s):\n","    # [s]会加一维，是list，通过np变成nparray，然后标明dtype\n","    return self.actor(np.array([s], dtype=np.float32))[0]\n","\n","  def learn(self):\n","    indices = np.random.choice(MEMORY_CAPACITY, size=BATCH_SIZE)\n","    bt = self.memory[indices, :]\n","    bs = bt[:, :self.s_dim]\n","    ba = bt[:, self.s_dim:self.s_dim + self.a_dim]\n","    br = bt[:, -self.s_dim - 1:-self.s_dim]\n","    bs_ = bt[:, -self.s_dim:]\n","\n","    # Critic\n","    with tf.GradientTape() as tape:\n","      a_ = self.actor_target(bs_)\n","      q_ = self.critic_target([bs_, a_])\n","      y = br + GAMMA * q_\n","      q = self.critic([bs, ba])\n","      td_error = tf.losses.mean_squared_error(y, q)\n","    c_grads = tape.gradient(td_error, self.critic.trainable_weights)\n","    self.critic_opt.apply_gradients(zip(c_grads, self.critic.trainable_weights))\n","\n","    # Actor\n","    with tf.GradientTape() as tape:\n","      a = self.actor(bs)\n","      q = self.critic([bs, a])\n","      a_loss = -tf.reduce_mean(q)\n","    a_grad = tape.gradient(a_loss, self.actor.trainable_weights)\n","    self.actor_opt.apply_gradients(zip(a_grad, self.actor.trainable_weights))\n","\n","    self.ema_update()\n","\n","  def store_transition(self, s, a, r, s_):\n","    s = s.astype(np.float32)\n","    s_ = s_.astype(np.float32)\n","    # 这里r是个标量，不是list，所以加[]，hstack水平合并\n","    transition = np.hstack((s, a, [r], s_))\n","    index = self.pointer % MEMORY_CAPACITY\n","    self.memory[index, :] = transition\n","    self.pointer += 1\n","\n","\n","  def save_ckpt(self):\n","    if not os.path.exists('model'):\n","      os.makedirs('model')\n","    tl.files.save_weights_to_hdf5('model/ddpg_actor.hdf5', self.actor)\n","    tl.files.save_weights_to_hdf5('model/ddpg_actor_target.hdf5', self.actor_target)\n","    tl.files.save_weights_to_hdf5('model/ddpg_critic.hdf5', self.critic)\n","    tl.files.save_weights_to_hdf5('model/ddpg_critic_target.hdf5', self.critic_target)\n","\n","\n","  def load_ckpt(self):\n","    tl.files.load_hdf5_to_weights_in_order('model/ddpg_actor.hdf5', self.actor)\n","    tl.files.load_hdf5_to_weights_in_order('model/ddpg_actor_target.hdf5', self.actor_target)\n","    tl.files.load_hdf5_to_weights_in_order('model/ddpg_critic.hdf5', self.critic)\n","    tl.files.load_hdf5_to_weights_in_order('model/ddpg_critic_target.hdf5', self.critic_target)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iTaFyrYJAf6D","colab_type":"text"},"source":["### **Main （初始化，训练，测试）**"]},{"cell_type":"code","metadata":{"id":"DpPdwfHRAVQc","colab_type":"code","cellView":"code","outputId":"2c8e2cca-1c6d-4288-80ee-df9fd02e489c","executionInfo":{"status":"ok","timestamp":1567516993964,"user_tz":-480,"elapsed":7299,"user":{"displayName":"Steven Iris","photoUrl":"","userId":"04172472830736813201"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["# 初始化\n","env = gym.make(ENV_NAME)\n","env = env.unwrapped\n","\n","env.seed(RANDOMSEED)\n","np.random.seed(RANDOMSEED)\n","tf.random.set_seed(RANDOMSEED)\n","\n","s_dim = env.observation_space.shape[0]\n","a_dim = env.action_space.shape[0]\n","a_bound = env.action_space.high\n","\n","print(\"observation dimension: {}\".format(s_dim))\n","print(\"action high: {}\".format(a_bound))\n","print(\"actions dimension: {}\".format(a_dim))\n","\n","ddpg = DDPG(a_dim, s_dim, a_bound)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["observation dimension: 3\n","action high: [2.]\n","actions dimension: 1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aL2sfZMGYSck","colab_type":"code","colab":{}},"source":["# train\n","reward_buffer = []\n","t0 = time.time()\n","for i in range(MAX_EPISODES):\n","  t1 = time.time()\n","  s = env.reset()\n","  ep_reward = 0\n","  for j in range(MAX_EP_STEPS):\n","    a = ddpg.choose_action(s)\n","    # 加noise 提高explore\n","    # np.random.normal(loc=均值，scale=标准差，size=None[输出个数，默认为1])\n","    a = np.clip(np.random.normal(a, VAR), -2, 2)\n","    s_, r, done, info = env.step(a)\n","\n","    ddpg.store_transition(s, a, r / 10, s_)\n","\n","    if ddpg.pointer > MEMORY_CAPACITY:\n","      ddpg.learn()\n","\n","    s = s_\n","    ep_reward += r\n","    if j == MAX_EP_STEPS - 1:\n","      print(\n","          '\\rEpisode:{}/{} | Episode Reward:{:.4f} | Running Time:{:.4f}'\\\n","          .format(i, MAX_EPISODES, ep_reward, time.time() - t1), end=''\n","      )\n","      # end='' 刷新式输出\n","    plt.show()\n","  \n","  # 训练中途测试\n","  if i and not i % TEST_PER_EPISODES:\n","    t1 = time.time()\n","    s = env.reset()\n","    ep_reward = 0\n","    for j in range(MAX_EP_STEPS):\n","      # 测试时不加noise\n","      a = ddpg.choose_action(s)\n","      s_, r, done, info = env.step(a)\n","\n","      s = s_\n","      ep_reward += r\n","      if j == MAX_EP_STEPS - 1:\n","        print(\n","          '\\rEpisode:{}/{} | Episode Reward:{:.4f} | Running Time:{:.4f}'\\\n","          .format(i, MAX_EPISODES, ep_reward, time.time() - t1)\n","        )      \n","        reward_buffer.append(ep_reward)\n","\n","  if reward_buffer:\n","    plt.ion()\n","    plt.cla()\n","    plt.title('DDPG')\n","    plt.plot(np.array(range(len(reward_buffer))) * TEST_PER_EPISODES, reward_buffer)\n","    plt.xlabel('episode steps')\n","    plt.ylabel('normalized state-action value')\n","    plt.ylim(-2000, 0)\n","    plt.show()\n","    plt.pause(0.1)\n","\n","plt.ioff()\n","plt.show()\n","print('\\nRunning time: ', time.time() - t0)\n","ddpg.save_ckpt()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qW5fmRz0wXN4","colab_type":"code","colab":{}},"source":["# test\n","ddpg.load_ckpt()\n","while True:\n","  s = env.reset()\n","  for i in range(MAX_EP_STEPS):\n","    # env.render()\n","    s, r, done, info = env.step(ddpg.choose_action(s))\n","    if done:\n","      print('finished one complete time')\n","      break"],"execution_count":0,"outputs":[]}]}