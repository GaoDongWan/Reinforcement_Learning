{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DQN_TF_PER_Varients.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"hB-PL2kOuMhx","colab_type":"text"},"source":["### **装载云盘**"]},{"cell_type":"code","metadata":{"id":"W3kk56zyuKF8","colab_type":"code","outputId":"a32024a0-1f15-4b23-9859-d97af8a47673","executionInfo":{"status":"ok","timestamp":1567470843597,"user_tz":-480,"elapsed":57772,"user":{"displayName":"Steven Iris","photoUrl":"","userId":"04172472830736813201"}},"colab":{"base_uri":"https://localhost:8080/","height":127}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ipnfS2Csyu33","colab_type":"text"},"source":["### **安装tensorflow 2.0**"]},{"cell_type":"code","metadata":{"id":"49C0mzzby3s_","colab_type":"code","colab":{}},"source":["!pip install tensorflow-gpu==2.0.0-beta1 "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SOpdHh4dAK0w","colab_type":"code","colab":{}},"source":["# tensorlayer 兼容问题\n","!pip install imgaug==0.2.6"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nDeaUCyG_kbP","colab_type":"code","colab":{}},"source":["!pip install tensorlayer"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wgNs2Gd10f73","colab_type":"text"},"source":["### **cd命令**"]},{"cell_type":"code","metadata":{"id":"BBQU9lh_yls5","colab_type":"code","colab":{}},"source":["import os\n","os.chdir(\"/content/drive/My Drive/RL_EA/Deep_Q_Network\")\n","# os.chdir(\"/usr/local/cuda/include/\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3TpgOs8f0lUO","colab_type":"text"},"source":["### **查看当前路径**"]},{"cell_type":"code","metadata":{"id":"c21teJ570bmB","colab_type":"code","outputId":"5782e6d5-305a-4b49-a499-db1cd25b718d","executionInfo":{"status":"ok","timestamp":1567475949192,"user_tz":-480,"elapsed":6950,"user":{"displayName":"Steven Iris","photoUrl":"","userId":"04172472830736813201"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["!pwd\n","# !cat /usr/local/cuda/version.txt\n","# !cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2\n","# !nvidia-smi"],"execution_count":12,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/RL_EA/Deep_Q_Network\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kWi3CPrl1q79","colab_type":"text"},"source":["# Deep Q Network\n","\n","$$\n","off-Policy\\\\\n","discrete\n","$$ \n","\n","两个特点：\n","\n","1）从 $Experience\\ Replay\\ Memory$ 中均匀采样。为了消除样本相关性。\n","\n","2）延迟复制权值的 $Fixed\\ Q-target$ 网络。提高稳定性，收敛性，消除估计$Q$和目标$Q$的相关性。\n","\n","回顾$Q-Learning$的更新过程:\n","\n","$$\n","Q(s,a)=Q(s,a)+\\alpha\\big[r+\\gamma\\max_{a'}Q(s',a')-Q(s,a)\\big]\n","$$\n","\n","可以看到$Q-Learning$就是让$Q$值接近目标$Q$值，那么$DQN$的$loss$：\n","\n","$$\n","targetQ\\:or\\:y'=r + \\gamma \\max_{a'} Q(s',a';\\theta_i^-)\n","$$\n","$$\n","L_i(\\theta_i) = \\mathbb{E}_{(s,a,r,s') \\sim U(D)} \\big[ \\big( r + \\gamma \\max_{a'} Q(s',a';\\theta_i^-) - Q(s, a; \\theta_i) \\big)^2 \\big]\n","$$\n","\n","## DQN的变种：\n","\n","### Double DQN：\n","$$\n","Y_t^{DoubleQ} = R_{t+1} + \\gamma  Q(S_{t+1}, \\arg\\max_a Q(S_{t+1}, a; \\theta_t); \\theta_t)\n","$$\n","减少过估计带来的影响。\n","\n","### Dueling DQN：\n","$$\n","Q(s, a; \\theta, \\alpha, \\beta) = V (s; \\theta, \\beta) + \\big( A(s, a; \\theta, \\alpha) - \\frac{1}{|\\mathcal{A}|} \\sum_{a'} A(s, a'; \\theta, \\alpha) \\big)\n","$$\n","同一个网络输出状态值和优势值。\n","\n","### Prioritized ER: \n","\n","是改进效果最好的一个DQN变种。按照 $\\delta_i=TD-error$ 的值的大小对ER中的经验进行排序，每个经验的概率为\n","\n","$$\n","P(i) = \\frac{p_i^{\\alpha}}{\\sum_k p_k^{\\alpha}}\n","$$\n","\n","其中 $p_i = |\\delta_i| + \\epsilon$。考虑到不能只学$P(i)$大的样本，导致过拟合，因此对其进行加权：\n","\n","$$\n","w_i = \\big( \\frac{1}{N} \\cdot \\frac{1}{P(i)} \\big)^\\beta\n","$$\n","\n","### Noisy DQN：\n","对网络中的某些（或全部）权值加上noise。也是为了防过拟合，过相关。\n","\n"]},{"cell_type":"code","metadata":{"id":"jFR7KnkqxMy-","colab_type":"code","colab":{}},"source":["# 实现prioritized+DQN\n","import time\n","import os\n","import random\n","import operator\n","\n","import numpy as np\n","\n","import gym\n","import tensorflow as tf\n","import tensorlayer as tl\n","from tutorial_wrappers import build_env\n","\n","random.seed(0)\n","np.random.seed(0)\n","tf.random.set_seed(0)\n","\n","ENV_NAME = 'CartPole-v0'  # PongNoFrameskip-v4 or CartPole-v0\n","env = build_env(ENV_NAME)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"EhurBgWjYqPH","colab":{}},"source":["### hyper ###\n","\n","if ENV_NAME == 'CartPole-v0':\n","  qnet_type = 'MLP'\n","  number_timesteps = 10000\n","  explore_timesteps = 100\n","  # 利用率从0.01 -> 0.99  有些python版本里要改成 1.0*i_iter\n","  epsilon = lambda i_iter: 1 - 0.99 * min(1, i_iter / explore_timesteps)\n","  lr = 5e-3\n","  buffer_size = 1000\n","  target_q_update_freq = 50\n","  ob_scale = 1.0\n","else:\n","  qnet_type = 'CNN'\n","  number_timesteps = int(1e6)\n","  explore_timesteps = 1e5\n","  # 利用率从0.01 -> 0.99\n","  epsilon = lambda i_iter: 1 - 0.99 * min(1, i_iter / explore_timesteps)\n","  lr = 1e-4\n","  buffer_size = 10000\n","  target_q_update_freq = 200\n","  ob_scale = 1.0 / 255\n","\n","in_dim = env.observation_space.shape\n","out_dim = env.action_space.n\n","reward_gamma = 0.99\n","batch_size = 32\n","warm_start = buffer_size / 10\n","prioritized_replay_alpha = 0.6\n","prioritized_replay_beta0 = 0.4\n","\n","summary_writer = tf.summary.create_file_writer(\"logs/\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ubTvO_ekY3g-","colab_type":"text"},"source":["### **网络框架**"]},{"cell_type":"code","metadata":{"id":"C3v7rblzY7LP","colab_type":"code","colab":{}},"source":["# 动态构建网络方式\n","class MLP(tl.models.Model):\n","\n","  def __init__(self, name):\n","    super(MLP, self).__init__(name=name)\n","    self.h1 = tl.layers.Dense(64, tf.nn.tanh, in_channels=in_dim[0])\n","    self.qvalue = tl.layers.Dense(out_dim, in_channels=64, name='q', W_init=tf.initializers.GlorotUniform())\n","    self.svalue = tl.layers.Dense(1, in_channels=64, name='s', W_init=tf.initializers.GlorotUniform())\n","\n","  def forward(self, ni):\n","    feature = self.h1(ni)\n","\n","    qvalue = self.qvalue(feature)\n","    svalue = self.svalue(feature)\n","    # Dueling DQN 公式\n","    out = svalue + qvalue - tf.reduce_mean(qvalue, 1, keepdims=True)\n","    return out\n","\n","\n","class CNN(tl.models.Model):\n","\n","  def __init__(self, name):\n","    super(CNN, self).__init__(name=name)\n","    h, w, in_channels = in_dim\n","    dense_in_channels = 64 * ((h - 28) // 8) * ((w - 28) // 8)\n","    self.conv1 = tl.layers.Conv2d(\n","        32, (8, 8), (4, 4), tf.nn.relu, 'VALID', in_channels=in_channels, name='conv2d_1',\n","        W_init=tf.initializers.GlorotUniform()\n","    )\n","    self.conv2 = tl.layers.Conv2d(\n","        64, (4, 4), (2, 2), tf.nn.relu, 'VALID', in_channels=32, name='conv2d_2',\n","        W_init=tf.initializers.GlorotUniform()\n","    )\n","    self.conv3 = tl.layers.Conv2d(\n","        64, (3, 3), (1, 1), tf.nn.relu, 'VALID', in_channels=64, name='conv2d_3',\n","        W_init=tf.initializers.GlorotUniform()\n","    )\n","    # 展开成vector\n","    self.flatten = tl.layers.Flatten(name='flatten')\n","    self.preq = tl.layers.Dense(\n","        256, tf.nn.relu, in_channels=dense_in_channels, name='pre_q', W_init=tf.initializers.GlorotUniform()\n","    )\n","    self.qvalue = tl.layers.Dense(out_dim, in_channels=256, name='q', W_init=tf.initializers.GlorotUniform())\n","    self.pres = tl.layers.Dense(\n","        256, tf.nn.relu, in_channels=dense_in_channels, name='pre_s', W_init=tf.initializers.GlorotUniform()\n","    )\n","    self.svalue = tl.layers.Dense(1, in_channels=256, name='s', W_init=tf.initializers.GlorotUniform())\n","\n","\n","  def forward(self, ni):\n","    feature = self.flatten(self.conv3(self.conv2(self.conv1(ni))))\n","    # preq 和 pres 分别是 V和A的NN层\n","    qvalue = self.qvalue(self.preq(feature))\n","    svalue = self.svalue(self.pres(feature))\n","    # Dueling DQN 公式\n","    out = svalue + qvalue - tf.reduce_mean(qvalue, 1, keepdims=True)\n","    return out\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vs_SZwfGhMhD","colab_type":"text"},"source":["### **$Priorizised\\ \\ ER$ 用到的线段树结构**"]},{"cell_type":"code","metadata":{"id":"NyyGM2XWhihU","colab_type":"code","colab":{}},"source":["class SegmentTree:\n","\n","  def __init__(self, capacity, operation, neutral_element):\n","    \"\"\"\n","    capacity: array的size，2的幂\n","    operation: 整合elements的操作\n","    neutral_element: 对上述操作的边界element\n","    \"\"\"\n","    assert capacity > 0 and capacity & (capacity - 1) == 0,\\\n","    \"capacity must be positive and a power of 2\"\n","    self._capacity = capacity\n","    # 完全二叉树的结构存储数据，叶子节点是数据值（这里是优先权重P）\n","    self._value = [neutral_element for _ in range(2 * capacity)]\n","    self._operation = operation\n","\n","  def _reduce_helper(self, start, end, node, node_start, node_end):\n","    # logN 操作复杂度\n","    if start == node_start and end == node_end:\n","      return self._value[node]\n","    mid = (node_start + node_end) // 2\n","    if end <= mid:\n","      return self._reduce_helper(start, end, 2 * node, node_start, mid)\n","    else:\n","      if mid + 1 <= start:\n","        return self._reduce_helper(start, end, 2 * node + 1, mid + 1, node_end)\n","      else:\n","        return self._operation(\n","            self._reduce_helper(start, mid, 2 * node, node_start, mid),\n","            self._reduce_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end)\n","        )\n","  \n","  def reduce(self, start=0, end=None):\n","    \"\"\"\n","    返回一个邻近子区间array\n","    \"\"\"\n","    if end is None:\n","      end = self._capacity\n","    if end < 0:\n","      end += self._capacity #???\n","    end -= 1\n","    return self._reduce_helper(start, end, 1, 0, self._capacity - 1)\n","\n","  def __setitem__(self, idx, val):\n","    # 叶子的index，类似java的get，set，这样做可以直接用数组形式访问。\n","    idx += self._capacity\n","    self._value[idx] = val\n","    idx //= 2\n","    while idx >= 1:\n","      self._value[idx] = self._operation(self._value[2 * idx], self._value[2 * idx + 1])\n","      idx //= 2\n","\n","  def __getitem__(self, idx):\n","    assert 0 <= idx < self._capacity\n","    return self._value[self._capacity + idx]\n","\n","# Sum 线段树\n","class SumSegmentTree(SegmentTree):\n","\n","  def __init__(self, capacity):\n","    super(SumSegmentTree, self).__init__(capacity=capacity, operation=operator.add, neutral_element=0.0)\n","\n","  def sum(self, start=0, end=None):\n","    \"\"\"\n","    return: arr[start]+...+arr[end]\n","    \"\"\"  \n","    return super(SumSegmentTree, self).reduce(start, end)\n","\n","  def find_prefixsum_idx(self, prefixsum):\n","    \"\"\"\n","    找到最大的index i，满足sum(arr[0]+...+arr[i-1]<=prefixsum)\n","\n","    如果array里是概率，这个函数可以从离散概率中sample index\n","    \n","    prefixsum: 之前的elements的和的上界\n","    \"\"\"\n","    assert 0 <= prefixsum <= self.sum() + 1e-5\n","    idx = 1\n","    while idx < self._capacity:\n","      if self._value[2 * idx] > prefixsum:\n","        idx = 2 * idx\n","      else:\n","        prefixsum -= self._value[2 * idx]\n","        idx = 2 * idx + 1\n","    return idx - self._capacity #???\n","\n","# Min线段树\n","class MinSegmentTree(SegmentTree):\n","\n","  def __init__(self, capacity):\n","    # 实际上不用Min树也行，直接对ER池的线段树求min。\n","    super(MinSegmentTree, self).__init__(capacity=capacity, operation=min, neutral_element=float('inf'))\n","\n","  def min(self, start=0, end=None):\n","    \"\"\"\n","    返回最小的arr[i]\n","    \"\"\"\n","    return super(MinSegmentTree, self).reduce(start, end)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i8R7rBo6ug50","colab_type":"text"},"source":["### **$Experience\\ \\ ReplayBuffer$**"]},{"cell_type":"code","metadata":{"id":"yv747O0Yup4a","colab_type":"code","colab":{}},"source":["class ReplayBuffer:\n","\n","  def __init__(self, size):\n","    self._storage = []\n","    self._maxsize = size\n","    self._next_idx = 0\n","\n","  def __len__(self):\n","    return len(self._storage)\n","\n","  def add(self, *args):\n","    if self._next_idx >= len(self._storage):\n","      self._storage.append(args)\n","    else:\n","      self._storage[self._next_idx] = args\n","    self._next_idx = (self._next_idx + 1) % self._maxsize\n","\n","  def _encode_sample(self, idxes):\n","    b_o, b_a, b_r, b_o_, b_d = [], [], [], [], []\n","    for i in idxes:\n","      o, a, r, o_, d = self._storage[i]\n","      b_o.append(o)\n","      b_a.append(a)\n","      b_r.append(r)\n","      b_o_.append(o_)\n","      b_d.append(d)\n","    return (\n","        np.stack(b_o).astype('float32') * ob_scale,\n","        np.stack(b_a).astype('int32'),\n","        np.stack(b_r).astype('float32'),\n","        np.stack(b_o_).astype('float32') * ob_scale,\n","        np.stack(b_d).astype('bool'),\n","    )\n","\n","  def sample(self, batch_size):\n","    indexes = range(len(self._storage))\n","    idxes = [random.choice(indexes) for _ in range(batch_size)]\n","    return self._encode_sample(idxes)\n","\n","# prioritized ER\n","class PrioritizedReplayBuffer(ReplayBuffer):\n","\n","  def __init__(self, size, alpha, beta):\n","    \"\"\"\n","    alpha: 0=不优先，1=完全优先\n","    \"\"\"\n","    super(PrioritizedReplayBuffer, self).__init__(size)\n","    assert alpha >= 0\n","    self._alpha = alpha\n","  \n","    it_capacity = 1\n","    while it_capacity < size:\n","      it_capacity *= 2\n","\n","    self._it_sum = SumSegmentTree(it_capacity)\n","    self._it_min = MinSegmentTree(it_capacity)\n","    self._max_priority = 1.0\n","    self.beta = beta\n","\n","  def add(self, *args):\n","    idx = self._next_idx\n","    super().add(*args)\n","    # 新加入的数据，给它最大的优先权，防止新数据不会被学习。 \n","    self._it_sum[idx] = self._max_priority**self._alpha\n","    self._it_min[idx] = self._max_priority**self._alpha\n","\n","  def _sample_proportinal(self, batch_size):\n","    res = []\n","    p_total = self._it_sum.sum(0, len(self._storage) - 1)\n","    every_range_len = p_total / batch_size\n","    for i in range(batch_size):\n","      mass = random.random() * every_range_len + i * every_range_len\n","      idx = self._it_sum.find_prefixsum_idx(mass)\n","      res.append(idx)\n","    return res\n","\n","  def sample(self, batch_size):\n","    idxes = self._sample_proportinal(batch_size)\n","\n","    it_sum = self._it_sum.sum()\n","    p_min = self._it_min.min() / it_sum # p_min 是为了计算 w_max\n","    max_weight = (p_min * len(self._storage))**(-self.beta) # w_max 是为了scale w(i) 见公式\n","\n","    p_samples = np.asarray([self._it_sum[idx] for idx in idxes]) / it_sum # 求p(i)\n","    weights = (p_samples * len(self._storage))**(-self.beta) / max_weight # 求w(i)\n","    encoded_sample = self._encode_sample(idxes)\n","    return encoded_sample + (weights, idxes)\n","\n","  def update_priorities(self, idxes, priorities):\n","    assert len(idxes) == len(priorities)\n","    for idx, priority in zip(idxes, priorities):\n","      assert priority > 0\n","      assert 0 <= idx < len(self._storage)\n","      self._it_sum[idx] = priority**self._alpha\n","      self._it_min[idx] = priority**self._alpha\n","\n","      self._max_priority = max(self._max_priority, priority)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xpCcuNHmCxmu","colab_type":"text"},"source":["### **$Utils$**"]},{"cell_type":"code","metadata":{"id":"I_OodosxDA2t","colab_type":"code","colab":{}},"source":["def huber_loss(x):\n","  return tf.where(tf.abs(x) < 1, tf.square(x) * 0.5, tf.abs(x) - 0.5)\n","\n","def sync(net, net_tar):\n","  for var, var_tar in zip(net.trainable_weights, net_tar.trainable_weights):\n","    var_tar.assign(var)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GdOfLRFyESDO","colab_type":"text"},"source":["### **训练**"]},{"cell_type":"code","metadata":{"id":"SiHqSS27ETwM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"4993dc57-8271-41c0-d887-61702cbd0912","executionInfo":{"status":"ok","timestamp":1567476746066,"user_tz":-480,"elapsed":200655,"user":{"displayName":"Steven Iris","photoUrl":"","userId":"04172472830736813201"}}},"source":["# train\n","qnet = MLP('q') if qnet_type == 'MLP' else CNN('q')\n","qnet.train()\n","trainable_weights = qnet.trainable_weights\n","targetqnet = MLP('targetq') if qnet_type == 'MLP' else CNN('targetq')\n","targetqnet.infer()\n","sync(qnet, targetqnet)\n","optimizer = tf.optimizers.Adam(learning_rate=lr)\n","\n","'''\n","这个buffer的线段树是有2n个点的，结构是完全二叉树，前n-1个点是后n个点的sum，第0个点不用，实际访问数是访问后n个数，\n","寻找数的时候从1开始，依次*2 或*2+1 访问子节点。\n","'''\n","buffer = PrioritizedReplayBuffer(buffer_size, prioritized_replay_alpha, prioritized_replay_beta0)\n","\n","o = env.reset()\n","nepisode = 0\n","t = time.time()\n","for i in range(1, number_timesteps + 1):\n","  eps = epsilon(i)\n","  buffer.beta += (1 - prioritized_replay_beta0) / number_timesteps\n","  \n","  # 选择动作\n","  if random.random() < eps:\n","    a = int(random.random() * out_dim)\n","  else:\n","    obv = np.expand_dims(o, 0).astype('float32') * ob_scale\n","    a = qnet(obv).numpy().argmax(1)[0]\n","\n","  # 执行动作，保存经验数据\n","  o_, r, done, info = env.step(a)\n","  buffer.add(o, a, r, o_, done)\n","\n","  if i >= warm_start:\n","    # 延迟更新target\n","    if i % target_q_update_freq == 0:\n","      sync(qnet, targetqnet)\n","      \n","      if i % 5000 == 0:\n","        path = os.path.join('model/', '{}_{}.npz'.format(qnet_type, i))\n","        tl.files.save_npz(qnet.trainable_weights, name=path)\n","\n","    # sample数据\n","    b_o, b_a, b_r, b_o_, b_d, weights, idxs = buffer.sample(batch_size)\n","\n","    #q' 估计\n","    b_q_ = (1 - b_d) * tf.reduce_max(targetqnet(b_o_), 1)\n","\n","    # loss\n","    with tf.GradientTape() as q_tape:\n","      b_q = tf.reduce_sum(qnet(b_o) * tf.one_hot(b_a, out_dim), 1)\n","      abs_td_error = tf.abs(b_q - (b_r + reward_gamma * b_q_))\n","      priorities = np.clip(abs_td_error.numpy(), 1e-6, None)\n","      buffer.update_priorities(idxs, priorities)\n","      loss = tf.reduce_mean(weights * huber_loss(abs_td_error))\n","\n","    q_grad = q_tape.gradient(loss, trainable_weights)\n","    optimizer.apply_gradients(zip(q_grad, trainable_weights))\n","\n","  if done:\n","    o = env.reset()\n","  else:\n","    o = o_\n","\n","  # episode in info is real (unwrapped) message ???\n","  if info.get('episode'):\n","    nepisode += 1\n","    reward, length = info['episode']['r'], info['episode']['l']\n","    fps = int(length / (time.time() - t))\n","    print(\n","        'Time steps fo far:{}, episode so fa:{}, episode reward:{:.4f}, episode length:{}, FPS:{}'\\\n","        .format(i, nepisode, reward, length, fps)\n","    )\n","    t = time.time()\n","    with summary_writer.as_default():\n","      tf.summary.scalar('reward', reward, step=nepisode)\n"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Time steps fo far:21, episode so fa:1, episode reward:21.0000, episode length:21, FPS:125\n","Time steps fo far:39, episode so fa:2, episode reward:18.0000, episode length:18, FPS:3612\n","Time steps fo far:49, episode so fa:3, episode reward:10.0000, episode length:10, FPS:1116\n","Time steps fo far:60, episode so fa:4, episode reward:11.0000, episode length:11, FPS:1062\n","Time steps fo far:69, episode so fa:5, episode reward:9.0000, episode length:9, FPS:1310\n","Time steps fo far:81, episode so fa:6, episode reward:12.0000, episode length:12, FPS:1161\n","Time steps fo far:90, episode so fa:7, episode reward:9.0000, episode length:9, FPS:1556\n","Time steps fo far:100, episode so fa:8, episode reward:10.0000, episode length:10, FPS:140\n","Time steps fo far:109, episode so fa:9, episode reward:9.0000, episode length:9, FPS:49\n","Time steps fo far:118, episode so fa:10, episode reward:9.0000, episode length:9, FPS:51\n","Time steps fo far:126, episode so fa:11, episode reward:8.0000, episode length:8, FPS:50\n","Time steps fo far:134, episode so fa:12, episode reward:8.0000, episode length:8, FPS:49\n","Time steps fo far:145, episode so fa:13, episode reward:11.0000, episode length:11, FPS:50\n","Time steps fo far:155, episode so fa:14, episode reward:10.0000, episode length:10, FPS:50\n","Time steps fo far:163, episode so fa:15, episode reward:8.0000, episode length:8, FPS:49\n","Time steps fo far:171, episode so fa:16, episode reward:8.0000, episode length:8, FPS:50\n","Time steps fo far:181, episode so fa:17, episode reward:10.0000, episode length:10, FPS:51\n","Time steps fo far:210, episode so fa:18, episode reward:29.0000, episode length:29, FPS:49\n","Time steps fo far:219, episode so fa:19, episode reward:9.0000, episode length:9, FPS:51\n","Time steps fo far:259, episode so fa:20, episode reward:40.0000, episode length:40, FPS:49\n","Time steps fo far:269, episode so fa:21, episode reward:10.0000, episode length:10, FPS:51\n","Time steps fo far:305, episode so fa:22, episode reward:36.0000, episode length:36, FPS:49\n","Time steps fo far:341, episode so fa:23, episode reward:36.0000, episode length:36, FPS:50\n","Time steps fo far:383, episode so fa:24, episode reward:42.0000, episode length:42, FPS:51\n","Time steps fo far:443, episode so fa:25, episode reward:60.0000, episode length:60, FPS:50\n","Time steps fo far:499, episode so fa:26, episode reward:56.0000, episode length:56, FPS:50\n","Time steps fo far:587, episode so fa:27, episode reward:88.0000, episode length:88, FPS:50\n","Time steps fo far:613, episode so fa:28, episode reward:26.0000, episode length:26, FPS:49\n","Time steps fo far:636, episode so fa:29, episode reward:23.0000, episode length:23, FPS:50\n","Time steps fo far:648, episode so fa:30, episode reward:12.0000, episode length:12, FPS:50\n","Time steps fo far:665, episode so fa:31, episode reward:17.0000, episode length:17, FPS:49\n","Time steps fo far:686, episode so fa:32, episode reward:21.0000, episode length:21, FPS:51\n","Time steps fo far:713, episode so fa:33, episode reward:27.0000, episode length:27, FPS:49\n","Time steps fo far:738, episode so fa:34, episode reward:25.0000, episode length:25, FPS:50\n","Time steps fo far:777, episode so fa:35, episode reward:39.0000, episode length:39, FPS:48\n","Time steps fo far:796, episode so fa:36, episode reward:19.0000, episode length:19, FPS:50\n","Time steps fo far:823, episode so fa:37, episode reward:27.0000, episode length:27, FPS:45\n","Time steps fo far:837, episode so fa:38, episode reward:14.0000, episode length:14, FPS:44\n","Time steps fo far:868, episode so fa:39, episode reward:31.0000, episode length:31, FPS:45\n","Time steps fo far:887, episode so fa:40, episode reward:19.0000, episode length:19, FPS:45\n","Time steps fo far:905, episode so fa:41, episode reward:18.0000, episode length:18, FPS:45\n","Time steps fo far:936, episode so fa:42, episode reward:31.0000, episode length:31, FPS:43\n","Time steps fo far:968, episode so fa:43, episode reward:32.0000, episode length:32, FPS:44\n","Time steps fo far:987, episode so fa:44, episode reward:19.0000, episode length:19, FPS:46\n","Time steps fo far:1020, episode so fa:45, episode reward:33.0000, episode length:33, FPS:43\n","Time steps fo far:1055, episode so fa:46, episode reward:35.0000, episode length:35, FPS:47\n","Time steps fo far:1106, episode so fa:47, episode reward:51.0000, episode length:51, FPS:50\n","Time steps fo far:1131, episode so fa:48, episode reward:25.0000, episode length:25, FPS:50\n","Time steps fo far:1172, episode so fa:49, episode reward:41.0000, episode length:41, FPS:50\n","Time steps fo far:1202, episode so fa:50, episode reward:30.0000, episode length:30, FPS:49\n","Time steps fo far:1235, episode so fa:51, episode reward:33.0000, episode length:33, FPS:50\n","Time steps fo far:1274, episode so fa:52, episode reward:39.0000, episode length:39, FPS:50\n","Time steps fo far:1343, episode so fa:53, episode reward:69.0000, episode length:69, FPS:49\n","Time steps fo far:1387, episode so fa:54, episode reward:44.0000, episode length:44, FPS:51\n","Time steps fo far:1433, episode so fa:55, episode reward:46.0000, episode length:46, FPS:51\n","Time steps fo far:1520, episode so fa:56, episode reward:87.0000, episode length:87, FPS:50\n","Time steps fo far:1584, episode so fa:57, episode reward:64.0000, episode length:64, FPS:50\n","Time steps fo far:1662, episode so fa:58, episode reward:78.0000, episode length:78, FPS:50\n","Time steps fo far:1769, episode so fa:59, episode reward:107.0000, episode length:107, FPS:50\n","Time steps fo far:1864, episode so fa:60, episode reward:95.0000, episode length:95, FPS:51\n","Time steps fo far:1958, episode so fa:61, episode reward:94.0000, episode length:94, FPS:50\n","Time steps fo far:2069, episode so fa:62, episode reward:111.0000, episode length:111, FPS:50\n","Time steps fo far:2183, episode so fa:63, episode reward:114.0000, episode length:114, FPS:51\n","Time steps fo far:2383, episode so fa:64, episode reward:200.0000, episode length:200, FPS:51\n","Time steps fo far:2526, episode so fa:65, episode reward:143.0000, episode length:143, FPS:51\n","Time steps fo far:2726, episode so fa:66, episode reward:200.0000, episode length:200, FPS:50\n","Time steps fo far:2926, episode so fa:67, episode reward:200.0000, episode length:200, FPS:51\n","Time steps fo far:3126, episode so fa:68, episode reward:200.0000, episode length:200, FPS:49\n","Time steps fo far:3326, episode so fa:69, episode reward:200.0000, episode length:200, FPS:45\n","Time steps fo far:3526, episode so fa:70, episode reward:200.0000, episode length:200, FPS:44\n","Time steps fo far:3726, episode so fa:71, episode reward:200.0000, episode length:200, FPS:50\n","Time steps fo far:3926, episode so fa:72, episode reward:200.0000, episode length:200, FPS:49\n","Time steps fo far:4126, episode so fa:73, episode reward:200.0000, episode length:200, FPS:50\n","Time steps fo far:4326, episode so fa:74, episode reward:200.0000, episode length:200, FPS:50\n","Time steps fo far:4526, episode so fa:75, episode reward:200.0000, episode length:200, FPS:50\n","Time steps fo far:4722, episode so fa:76, episode reward:196.0000, episode length:196, FPS:51\n","Time steps fo far:4913, episode so fa:77, episode reward:191.0000, episode length:191, FPS:50\n","Time steps fo far:5089, episode so fa:78, episode reward:176.0000, episode length:176, FPS:51\n","Time steps fo far:5256, episode so fa:79, episode reward:167.0000, episode length:167, FPS:50\n","Time steps fo far:5441, episode so fa:80, episode reward:185.0000, episode length:185, FPS:49\n","Time steps fo far:5620, episode so fa:81, episode reward:179.0000, episode length:179, FPS:50\n","Time steps fo far:5805, episode so fa:82, episode reward:185.0000, episode length:185, FPS:50\n","Time steps fo far:5996, episode so fa:83, episode reward:191.0000, episode length:191, FPS:50\n","Time steps fo far:6196, episode so fa:84, episode reward:200.0000, episode length:200, FPS:50\n","Time steps fo far:6396, episode so fa:85, episode reward:200.0000, episode length:200, FPS:50\n","Time steps fo far:6596, episode so fa:86, episode reward:200.0000, episode length:200, FPS:50\n","Time steps fo far:6796, episode so fa:87, episode reward:200.0000, episode length:200, FPS:50\n","Time steps fo far:6996, episode so fa:88, episode reward:200.0000, episode length:200, FPS:50\n","Time steps fo far:7196, episode so fa:89, episode reward:200.0000, episode length:200, FPS:50\n","Time steps fo far:7396, episode so fa:90, episode reward:200.0000, episode length:200, FPS:50\n","Time steps fo far:7596, episode so fa:91, episode reward:200.0000, episode length:200, FPS:50\n","Time steps fo far:7796, episode so fa:92, episode reward:200.0000, episode length:200, FPS:50\n","Time steps fo far:7996, episode so fa:93, episode reward:200.0000, episode length:200, FPS:50\n","Time steps fo far:8196, episode so fa:94, episode reward:200.0000, episode length:200, FPS:49\n","Time steps fo far:8396, episode so fa:95, episode reward:200.0000, episode length:200, FPS:51\n","Time steps fo far:8596, episode so fa:96, episode reward:200.0000, episode length:200, FPS:50\n","Time steps fo far:8796, episode so fa:97, episode reward:200.0000, episode length:200, FPS:50\n","Time steps fo far:8996, episode so fa:98, episode reward:200.0000, episode length:200, FPS:50\n","Time steps fo far:9196, episode so fa:99, episode reward:200.0000, episode length:200, FPS:51\n","Time steps fo far:9396, episode so fa:100, episode reward:200.0000, episode length:200, FPS:50\n","Time steps fo far:9596, episode so fa:101, episode reward:200.0000, episode length:200, FPS:50\n","Time steps fo far:9796, episode so fa:102, episode reward:200.0000, episode length:200, FPS:50\n","Time steps fo far:9996, episode so fa:103, episode reward:200.0000, episode length:200, FPS:50\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_zbcf_BDEUTP","colab_type":"text"},"source":["### **测试**"]},{"cell_type":"code","metadata":{"id":"WF7pQBi9EWiq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":917},"outputId":"28414567-8370-4e52-ccc5-2032c06c3cce","executionInfo":{"status":"ok","timestamp":1567476897725,"user_tz":-480,"elapsed":13055,"user":{"displayName":"Steven Iris","photoUrl":"","userId":"04172472830736813201"}}},"source":["# test\n","qnet = MLP('q_t0') if qnet_type == 'MLP' else CNN('q_t0')\n","# tl.files.load_and_assign_npz(name='model/{}.npz'.format(20800))\n","tl.files.load_and_assign_npz(name='model/CartPole_10000.npz', network=qnet)\n","qnet.eval()\n","\n","nepisode = 0\n","o = env.reset()\n","for i in range(1, number_timesteps + 1):\n","  # env.render()\n","  obv = np.expand_dims(o, 0).astype('float32') * ob_scale\n","  a = qnet(obv).numpy().argmax(1)[0]\n","   \n","  o_, r, done, info = env.step(a)\n","\n","  if done:\n","    o = env.reset()\n","  else:\n","    o = o_\n","  \n","  # ???\n","  if info.get('episode'):\n","    nepisode += 1\n","    reward, length = info['episode']['r'], info['episode']['l']\n","    print(\n","        'Time steps so fa:{}, episode so far:{}, episode reward:{:.4f}, episode length:{}'\\\n","        .format(i, nepisode, reward, length)\n","    )"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Time steps so fa:200, episode so far:1, episode reward:200.0000, episode length:200\n","Time steps so fa:400, episode so far:2, episode reward:200.0000, episode length:200\n","Time steps so fa:600, episode so far:3, episode reward:200.0000, episode length:200\n","Time steps so fa:800, episode so far:4, episode reward:200.0000, episode length:200\n","Time steps so fa:1000, episode so far:5, episode reward:200.0000, episode length:200\n","Time steps so fa:1200, episode so far:6, episode reward:200.0000, episode length:200\n","Time steps so fa:1400, episode so far:7, episode reward:200.0000, episode length:200\n","Time steps so fa:1600, episode so far:8, episode reward:200.0000, episode length:200\n","Time steps so fa:1800, episode so far:9, episode reward:200.0000, episode length:200\n","Time steps so fa:2000, episode so far:10, episode reward:200.0000, episode length:200\n","Time steps so fa:2200, episode so far:11, episode reward:200.0000, episode length:200\n","Time steps so fa:2400, episode so far:12, episode reward:200.0000, episode length:200\n","Time steps so fa:2600, episode so far:13, episode reward:200.0000, episode length:200\n","Time steps so fa:2800, episode so far:14, episode reward:200.0000, episode length:200\n","Time steps so fa:3000, episode so far:15, episode reward:200.0000, episode length:200\n","Time steps so fa:3200, episode so far:16, episode reward:200.0000, episode length:200\n","Time steps so fa:3400, episode so far:17, episode reward:200.0000, episode length:200\n","Time steps so fa:3600, episode so far:18, episode reward:200.0000, episode length:200\n","Time steps so fa:3800, episode so far:19, episode reward:200.0000, episode length:200\n","Time steps so fa:4000, episode so far:20, episode reward:200.0000, episode length:200\n","Time steps so fa:4200, episode so far:21, episode reward:200.0000, episode length:200\n","Time steps so fa:4400, episode so far:22, episode reward:200.0000, episode length:200\n","Time steps so fa:4600, episode so far:23, episode reward:200.0000, episode length:200\n","Time steps so fa:4800, episode so far:24, episode reward:200.0000, episode length:200\n","Time steps so fa:5000, episode so far:25, episode reward:200.0000, episode length:200\n","Time steps so fa:5200, episode so far:26, episode reward:200.0000, episode length:200\n","Time steps so fa:5400, episode so far:27, episode reward:200.0000, episode length:200\n","Time steps so fa:5600, episode so far:28, episode reward:200.0000, episode length:200\n","Time steps so fa:5800, episode so far:29, episode reward:200.0000, episode length:200\n","Time steps so fa:6000, episode so far:30, episode reward:200.0000, episode length:200\n","Time steps so fa:6200, episode so far:31, episode reward:200.0000, episode length:200\n","Time steps so fa:6400, episode so far:32, episode reward:200.0000, episode length:200\n","Time steps so fa:6600, episode so far:33, episode reward:200.0000, episode length:200\n","Time steps so fa:6800, episode so far:34, episode reward:200.0000, episode length:200\n","Time steps so fa:7000, episode so far:35, episode reward:200.0000, episode length:200\n","Time steps so fa:7200, episode so far:36, episode reward:200.0000, episode length:200\n","Time steps so fa:7400, episode so far:37, episode reward:200.0000, episode length:200\n","Time steps so fa:7600, episode so far:38, episode reward:200.0000, episode length:200\n","Time steps so fa:7800, episode so far:39, episode reward:200.0000, episode length:200\n","Time steps so fa:8000, episode so far:40, episode reward:200.0000, episode length:200\n","Time steps so fa:8200, episode so far:41, episode reward:200.0000, episode length:200\n","Time steps so fa:8400, episode so far:42, episode reward:200.0000, episode length:200\n","Time steps so fa:8600, episode so far:43, episode reward:200.0000, episode length:200\n","Time steps so fa:8800, episode so far:44, episode reward:200.0000, episode length:200\n","Time steps so fa:9000, episode so far:45, episode reward:200.0000, episode length:200\n","Time steps so fa:9200, episode so far:46, episode reward:200.0000, episode length:200\n","Time steps so fa:9400, episode so far:47, episode reward:200.0000, episode length:200\n","Time steps so fa:9600, episode so far:48, episode reward:200.0000, episode length:200\n","Time steps so fa:9800, episode so far:49, episode reward:200.0000, episode length:200\n","Time steps so fa:10000, episode so far:50, episode reward:200.0000, episode length:200\n"],"name":"stdout"}]}]}